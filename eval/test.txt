import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED: This function now correctly iterates over *all* content types
    (paragraphs, tables, lists, footnotes) from the Document Intelligence JSON.
    It uses each item's own 'content' field and sorts them by their
    character 'offset' to guarantee a complete and correctly ordered list.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        
        # Get all top-level content lists
        raw_paragraphs = analyze_result.get('paragraphs', [])
        raw_tables = analyze_result.get('tables', [])
        raw_lists = analyze_result.get('lists', [])
        raw_footnotes = analyze_result.get('footnotes', [])
        
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e
    except Exception as e:
        raise ValueError(f"Error parsing initial JSON structure: {e}") from e


    all_content: List[ContentItem] = []

    # --- Step 1: Process PARAGRAPHS (includes headings, subheadings, text) ---
    for p in raw_paragraphs:
        role = p.get('role', 'paragraph')
        if role in config.IGNORED_ROLES or not p.get('spans'):
            continue

        text = p.get('content', '').strip()
        offset = p['spans'][0]['offset']
        page = p.get('pageNumber', 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': role, 
                'page': page, 
                'offset': offset
            })

    # --- Step 2: Process TABLES ---
    # Note: A 'table' object's content is its markdown/HTML representation,
    # which is exactly what we want for a single block.
    for t in raw_tables:
        if not t.get('spans'):
            continue
        
        text = t.get('content', '').strip()
        offset = t['spans'][0]['offset']
        page = t.get('pageNumber', 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': 'table', 
                'page': page, 
                'offset': offset
            })

    # --- Step 3: Process LISTS ---
    for l in raw_lists:
        if not l.get('spans'):
            continue

        text = l.get('content', '').strip()
        offset = l['spans'][0]['offset']
        page = l.get('pageNumber', 0)

        if text:
            all_content.append({
                'text': text,
                'type': 'listItem', # Use a consistent role
                'page': page,
                'offset': offset
            })

    # --- Step 4: Process FOOTNOTES ---
    for f in raw_footnotes:
        if not f.get('spans'):
            continue

        text = f.get('content', '').strip()
        offset = f['spans'][0]['offset']
        page = f.get('pageNumber', 0)

        if text:
            all_content.append({
                'text': text,
                'type': 'footnote',
                'page': page,
                'offset': offset
            })

    # --- Step 5: De-duplicate content ---
    # The 'paragraphs' list sometimes includes content that is *also* in the
    # 'tables', 'lists', or 'footnotes' lists. We must de-duplicate.
    # The 'offset' is the unique key.
    
    seen_offsets = set()
    deduplicated_content = []
    
    # We sort by offset first to process in document order
    all_content.sort(key=lambda x: x['offset'])
    
    # We prioritize non-paragraph roles (table, list, footnote)
    # If a paragraph has the same offset, it's likely a duplicate.
    
    # First, add all special types and their offsets
    special_offsets = set()
    for item in all_content:
        if item['type'] not in ['paragraph', 'title', 'sectionHeading', 'subheading']:
            if item['offset'] not in seen_offsets:
                deduplicated_content.append(item)
                seen_offsets.add(item['offset'])
                special_offsets.add(item['offset'])

    # Now, add only the paragraphs that don't share an offset
    for item in all_content:
        if item['type'] in ['paragraph', 'title', 'sectionHeading', 'subheading']:
            if item['offset'] not in seen_offsets:
                deduplicated_content.append(item)
                seen_offsets.add(item['offset'])

    # --- Step 6: Final Sort ---
    # Re-sort the de-duplicated list to ensure perfect order
    deduplicated_content.sort(key=lambda x: x['offset'])
    
    print(f"JSON parser captured {len(deduplicated_content)} de-duplicated content items.")
    return deduplicated_content
