import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED (THE REAL FIX):
    This parser now does one job consistently:
    1. Gets *all* content elements (paragraphs, tables, etc.).
    2. Gets the `offset` and `length` of each element.
    3. Slices the main `analyzeResult.content` (the full markdown string)
       using that offset and length.
       
    This ensures that *all* content, including lists and footnotes
    (which are part of the 'paragraphs' list), is extracted correctly.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        # The full markdown string is the single source of truth
        full_text_content = analyze_result.get('content', '')
        if not full_text_content:
             raise ValueError("Document Intelligence 'content' field is empty.")
             
        raw_paragraphs = analyze_result.get('paragraphs', [])
        pages = analyze_result.get('pages', [])
        raw_tables = analyze_result.get('tables', [])
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    # --- Step 1: Create a page lookup for all items ---
    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    # --- Step 2: Extract all content items by slicing the main content string ---
    all_content: List[ContentItem] = []

    # Process PARAGRAPHS (includes sectionHeading, title, listItem, footnote, etc.)
    for p in raw_paragraphs:
        role = p.get('role', 'paragraph')
        if role in config.IGNORED_ROLES or not p.get('spans'):
            continue

        span = p['spans'][0]
        offset = span['offset']
        length = span['length']
        
        # FIX: Slice the main content string instead of using p.get('content')
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': role, 
                'page': page_number, 
                'offset': offset
            })

    # Process TABLES
    for table in raw_tables:
        if not table.get('spans'):
            continue
        
        span = table['spans'][0]
        offset = span['offset']
        length = span['length']

        # This logic was already correct: slice the main content string
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': 'table', 
                'page': page_number, 
                'offset': offset
            })

    # --- Step 3: Sort all extracted content by its character offset ---
    all_content.sort(key=lambda x: x['offset'])
    
    # We return the raw, sorted list of ALL items.
    return all_content
