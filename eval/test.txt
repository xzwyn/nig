from dotenv import load_dotenv
load_dotenv(override=True)

import streamlit as st
import os
from datetime import datetime

from pdf_processor import extract_structural_elements  # etc...
from aligner import align_documents
from agent import run_evaluation_pipeline
from ui_components import display_results, prepare_excel_download


# --- Page Configuration ---
st.set_page_config(page_title=" Translation Evaluator", layout="wide")

# --- UI ---
st.title(" Translation Evaluator")
st.markdown("Upload a source English PDF and its German translation to identify translation errors.")

# --- Session State Initialization ---
def init_session_state():
    st.session_state.setdefault('processing_started', False)
    st.session_state.setdefault('analysis_complete', False)
    st.session_state.setdefault('results', [])
    st.session_state.setdefault('aligned_pairs', [])

init_session_state()

# --- Helper Function ---
def save_uploaded_file(uploaded_file):
    os.makedirs("temp", exist_ok=True)
    file_path = os.path.join("temp", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

# --- Sidebar ---
with st.sidebar:
    st.header("1. Upload Documents")
    english_pdf = st.file_uploader("Upload English PDF (Source)", type="pdf")
    german_pdf = st.file_uploader("Upload German PDF (Translation)", type="pdf")

    st.header("2. Start Analysis")
    if st.button("Run Evaluation", disabled=st.session_state.processing_started or not (english_pdf and german_pdf)):
        # Reset state for a new run
        st.session_state.processing_started = True
        st.session_state.analysis_complete = False
        st.session_state.results = []
        st.session_state.aligned_pairs = []
        
        with st.spinner("Step 1/2: Processing and aligning documents..."):
            eng_path = save_uploaded_file(english_pdf)
            ger_path = save_uploaded_file(german_pdf)
            eng_elements = extract_structural_elements(eng_path, language="eng", strategy="fast")
            ger_elements = extract_structural_elements(ger_path, language="deu", strategy="fast")
            st.session_state.aligned_pairs = align_documents(eng_elements, ger_elements)
        # Streamlit will automatically re-run to start the processing loop.

    # --- CHANGED: Download Button Logic ---
    st.header("3. Export Results")
    # This logic now safely handles the download button creation
    if st.session_state.processing_started or st.session_state.analysis_complete:
        # Check if there are any results to download
        if st.session_state.results:
            excel_data = prepare_excel_download(st.session_state.results)
            current_date = datetime.now().strftime("%Y-%m-%d")
            st.download_button(
                label="ðŸ“¥ Download Report as Excel",
                data=excel_data,
                file_name=f"Translation_Analysis_{current_date}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                disabled=False
            )
        else:
            # If no results yet, show a disabled button to prevent crashing
            st.download_button(
                label="ðŸ“¥ Download Report as Excel",
                data=b'', # Use empty bytes as a placeholder
                file_name=f"Translation_Analysis.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                disabled=True
            )
    else:
        st.markdown("*(The download button will appear here once analysis begins.)*")
    # --- END OF CHANGE ---

# --- Main Display and Processing Area ---
st.header("Evaluation Results")

# This is the main processing block that runs after the button is pressed
if st.session_state.processing_started and not st.session_state.analysis_complete:
    total_pairs = len(st.session_state.aligned_pairs)
    st.info(f"Step 2/2: Evaluating {total_pairs} pairs...")
    progress_bar = st.progress(0)
    
    results_container = st.container()
    
    # Process all pairs using the generator and update the UI in a single block
    all_results = []
    for i, result in enumerate(run_evaluation_pipeline(st.session_state.aligned_pairs)):
        if result:
            all_results.append(result)
        
        # Update session state and UI components inside the loop
        st.session_state.results = all_results
        progress_bar.progress((i + 1) / total_pairs)
        
        with results_container:
            results_container.empty()
            display_results(st.session_state.results)

    # Mark as complete once the loop finishes
    st.session_state.analysis_complete = True
    st.session_state.processing_started = False # Allow for a new run
    st.rerun() # Rerun to update the button state and final messages

# This block displays the final state after processing is complete or before it starts
else:
    if st.session_state.analysis_complete:
        if not st.session_state.results:
            st.success("âœ… Analysis complete. No significant errors were found.")
        else:
            display_results(st.session_state.results) # Display final results
    else:
        st.info("Upload documents and click 'Run Evaluation' to begin.")



# ui_components.py

import streamlit as st
import pandas as pd
import io

def display_results(results_list: list):
    """
    Renders the list of findings, now with a dedicated section for
    displaying granular phrase-level errors.
    """
    if not results_list:
        st.info("No significant translation errors were found in the analysis.")
        return

    st.subheader(f"Found {len(results_list)} noteworthy items")
    results_list.sort(key=lambda x: x.get('page', 0))

    for result in results_list:
        error_type = result.get('type', 'Info')
        
        with st.container(border=True):
            st.markdown(f"**Page:** `{result.get('page', 'N/A')}` | **Type:** `{error_type}`")
            
            # --- NEW: Granular Display for Phrase-Level Errors ---
            original_phrase = result.get("original_phrase")
            translated_phrase = result.get("translated_phrase")

            if original_phrase and translated_phrase:
                st.markdown("##### ðŸ” Error Focus")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("Original English Phrase:")
                    st.error(f"'{original_phrase}'")
                with col2:
                    st.markdown("Translated German Phrase:")
                    st.warning(f"'{translated_phrase}'")
                st.divider()

            # Display full text for context, unless it's a table error
            if "Table Error" not in error_type:
                st.markdown("##### Full Text Context")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown(f"> {result['english_text']}")
                with col2:
                    st.markdown(f"> {result['german_text']}")

            st.markdown(f"**ðŸ’¡ Suggestion:** {result['suggestion']}")


def prepare_excel_download(results_list: list):
    """
    Converts the list of results into an Excel file in memory for downloading.
    Now includes the specific phrases.
    """
    if not results_list:
        return None

    df_data = {
        "Page": [r.get('page', 'N/A') for r in results_list],
        "Error Type": [r.get('type') for r in results_list],
        "Original Phrase": [r.get('original_phrase', 'N/A') for r in results_list],
        "Translated Phrase": [r.get('translated_phrase', 'N/A') for r in results_list],
        "Suggestion": [r.get('suggestion') for r in results_list],
        "Full English Source": [r.get('english_text') for r in results_list],
        "Full German Translation": [r.get('german_text') for r in results_list]
    }
    
    df = pd.DataFrame(df_data)
    
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Translation_Analysis')
    
    processed_data = output.getvalue()
    return processed_data




















"""
This code sample shows Prebuilt Layout operations with the Azure AI Document Intelligence client library.
The async versions of the samples require Python 3.8 or later.

To learn more, please visit the documentation - Quickstart: Document Intelligence (formerly Form Recognizer) SDKs
https://learn.microsoft.com/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?pivots=programming-language-python
"""

from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest

"""
Remember to remove the key from your code when you're done, and never post it publicly. For production, use
secure methods to store and access your credentials. For more information, see 
https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration
"""
endpoint = "YOUR_FORM_RECOGNIZER_ENDPOINT"
key = "YOUR_FORM_RECOGNIZER_KEY"

# sample document
formUrl = "https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf"

document_intelligence_client  = DocumentIntelligenceClient(
    endpoint=endpoint, credential=AzureKeyCredential(key)
)

poller = document_intelligence_client.begin_analyze_document(
    "prebuilt-layout", AnalyzeDocumentRequest(url_source=formUrl)
)
result = poller.result()

for idx, style in enumerate(result.styles):
    print(
        "Document contains {} content".format(
         "handwritten" if style.is_handwritten else "no handwritten"
        )
    )

for page in result.pages:
    for line_idx, line in enumerate(page.lines):
        print(
         "...Line # {} has text content '{}'".format(
        line_idx,
        line.content.encode("utf-8")
        )
    )

    for selection_mark in page.selection_marks:
        print(
         "...Selection mark is '{}' and has a confidence of {}".format(
         selection_mark.state,
         selection_mark.confidence
         )
    )

for table_idx, table in enumerate(result.tables):
    print(
        "Table # {} has {} rows and {} columns".format(
        table_idx, table.row_count, table.column_count
        )
    )
        
    for cell in table.cells:
        print(
            "...Cell[{}][{}] has content '{}'".format(
            cell.row_index,
            cell.column_index,
            cell.content.encode("utf-8"),
            )
        )

print("----------------------------------------")






