# test_1/requirements.txt
# Core application
streamlit
openai
azure-core
python-dotenv
azure-ai-documentintelligence
faiss-cpu
PyMuPDF

# Libraries for data handling and calculations
numpy
scikit-learn
tqdm

# Libraries for dataframes and writing Excel files
pandas
openpyxl



# test_1/src/processing/toc_parser.py
import re
from pathlib import Path
from typing import List, Dict, Any
import fitz  # PyMuPDF

# A type alias for a structured ToC item
ToCItem = Dict[str, Any]

def get_toc_text_from_pdf(pdf_path: Path, page_num: int = 1) -> str:
    """Extracts raw text from a specific page of a PDF file."""
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF file not found at: {pdf_path}")

    try:
        with fitz.open(pdf_path) as doc:
            if page_num < len(doc):
                page = doc.load_page(page_num)
                return page.get_text("text")
            else:
                # Fallback to first page if specified page doesn't exist
                print(f"Warning: Page {page_num} not found. Falling back to page 0.")
                page = doc.load_page(0)
                return page.get_text("text")
    except Exception as e:
        raise IOError(f"Error opening or reading PDF file '{pdf_path}': {e}") from e

def structure_toc(toc_text: str) -> List[ToCItem]:
    """
    Parses the raw text of a Table of Contents into a structured list of sections,
    including their titles and start pages.
    """
    # Regex to find lines ending with a number, optionally followed by junk to be stripped.
    # It captures the title and the page number.
    # Example: "2. Supervisory Board Report ......... 4"
    section_pattern = re.compile(r"^(.*?)\s*[.\s]+(\d+)\s*$")
    
    structured_list: List[ToCItem] = []
    lines = toc_text.split('\n')

    for line in lines:
        line = line.strip()
        if not line or len(line) < 3: # Ignore empty or very short lines
            continue
            
        match = section_pattern.match(line)
        if match:
            title, page_number_str = match.groups()
            
            # Further clean the title from leading numbers/bullets
            title = re.sub(r"^\s*([A-D]|\d{1,2}[\.\s])\s*[_]?\s*", "", title).strip()
            
            if title and len(title) > 4:  # Ensure title is meaningful
                structured_list.append({
                    'title': title,
                    'start_page': int(page_number_str)
                })

    if not structured_list:
        return []

    # Sort by start page to ensure correct order
    structured_list.sort(key=lambda x: x['start_page'])

    # Calculate the end_page for each section
    for i in range(len(structured_list) - 1):
        structured_list[i]['end_page'] = structured_list[i+1]['start_page'] - 1

    # Set end_page for the last section to a high number
    structured_list[-1]['end_page'] = 999

    return structured_list




# test_1/src/alignment/toc_aligner.py
from typing import List, Dict, Any
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.optimize import linear_sum_assignment
from src.clients.azure_client import get_embeddings # Uses the existing Azure client

# Type Aliases
ToCItem = Dict[str, Any]
AlignedToCPair = Dict[str, Any]

def align_tocs(english_toc: List[ToCItem], german_toc: List[ToCItem]) -> List[AlignedToCPair]:
    """
    Aligns Table of Contents sections using Azure OpenAI embeddings and the Hungarian algorithm.
    """
    if not english_toc or not german_toc:
        return []

    eng_titles = [item['title'] for item in english_toc]
    ger_titles = [item['title'] for item in german_toc]

    print("Getting embeddings for ToC titles...")
    english_embeddings = np.array(get_embeddings(eng_titles))
    german_embeddings = np.array(get_embeddings(ger_titles))

    similarity_matrix = cosine_similarity(english_embeddings, german_embeddings)

    # Use the Hungarian algorithm for optimal assignment
    cost_matrix = -similarity_matrix
    row_indices, col_indices = linear_sum_assignment(cost_matrix)

    aligned_sections: List[AlignedToCPair] = []
    print("Matching ToC sections...")
    for eng_idx, ger_idx in zip(row_indices, col_indices):
        score = similarity_matrix[eng_idx, ger_idx]
        # Use a reasonable threshold to filter out poor matches
        if score > 0.5:
            aligned_sections.append({
                'english': english_toc[eng_idx],
                'german': german_toc[ger_idx],
                'similarity': score
            })
    
    aligned_sections.sort(key=lambda x: x['english']['start_page'])
    return aligned_sections



# test_1/app.py
from dotenv import load_dotenv
load_dotenv()

import streamlit as st
from datetime import datetime
import time
from pathlib import Path
import os
from tqdm import tqdm

# --- Import project modules ---
import config
from src.clients.doc_intelligence_client import analyze_pdf
from src.processing.json_parser import process_document_json
from src.processing.toc_parser import get_toc_text_from_pdf, structure_toc
from src.alignment.toc_aligner import align_tocs
from src.alignment.semantic_aligner import align_content
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import create_excel_report_in_memory, save_alignment_report, save_evaluation_report
from src.reporting.markdown_writer import save_to_markdown

# --- Helper to save uploaded files temporarily ---
def save_temp_file(uploaded_file) -> Path:
    temp_dir = Path("./temp_uploads")
    temp_dir.mkdir(exist_ok=True)
    file_path = temp_dir / uploaded_file.name
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

# --- Page Configuration & UI Functions (unchanged) ---
st.set_page_config(page_title="Translation Evaluator", layout="wide")

def display_results(results_list: list):
    """Renders the list of evaluation findings in the Streamlit UI."""
    if not results_list: return
    st.subheader(f"Found {len(results_list)} noteworthy items")
    results_list.sort(key=lambda x: x.get('page', 0))
    for result in results_list:
        error_type = result.get('type', 'Info')
        with st.container(border=True):
            st.markdown(f"**Page:** `{result.get('page', 'N/A')}` | **Type:** `{error_type}`")
            original_phrase, translated_phrase = result.get("original_phrase"), result.get("translated_phrase")
            if original_phrase or translated_phrase:
                st.markdown("##### 🔍 Error Focus")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Original English Phrase:**"); st.error(f"'{original_phrase or 'N/A'}'")
                with col2:
                    st.markdown("**Translated German Phrase:**"); st.warning(f"'{translated_phrase or 'N/A'}'")
                st.divider()
            st.markdown("##### Full Text Context")
            col1, col2 = st.columns(2)
            with col1: st.markdown(f"> {result['english_text']}")
            with col2: st.markdown(f"> {result['german_text']}")
            st.markdown(f"**💡 Suggestion:** {result['suggestion']}")

# --- Main App ---
st.title("📚 Translation Evaluator (ToC-Based)")
st.markdown("This tool aligns documents section-by-section based on the Table of Contents for improved accuracy.")
st.divider()

if 'analysis_complete' not in st.session_state: st.session_state.analysis_complete = False
if 'evaluation_results' not in st.session_state: st.session_state.evaluation_results = []
if 'error_message' not in st.session_state: st.session_state.error_message = None

# --- Sidebar for Inputs and Controls ---
with st.sidebar:
    st.header("1. Upload Documents")
    english_pdf = st.file_uploader("Upload English PDF (Source)", type="pdf", key="eng_pdf")
    german_pdf = st.file_uploader("Upload German PDF (Translation)", type="pdf", key="ger_pdf")

    st.header("2. Configure & Run")
    toc_page_num = st.number_input("ToC Page Number in PDF", min_value=1, max_value=20, value=2) - 1
    
    if st.button("🚀 Run Analysis", disabled=not (english_pdf and german_pdf), type="primary"):
        st.session_state.analysis_complete = False
        st.session_state.evaluation_results = []
        st.session_state.error_message = None
        
        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{Path(english_pdf.name).stem}_{timestamp}"

        try:
            final_aligned_pairs = []
            
            with st.spinner("Step 1/7: Analyzing PDFs with Azure Document Intelligence..."):
                eng_json_data = analyze_pdf(english_pdf.getvalue(), english_pdf.name)
                ger_json_data = analyze_pdf(german_pdf.getvalue(), german_pdf.name)

            with st.spinner("Step 2/7: Processing full document content..."):
                full_english_content = process_document_json(eng_json_data)
                full_german_content = process_document_json(ger_json_data)

            with st.spinner("Step 3/7: Extracting and structuring Tables of Contents..."):
                eng_temp_path = save_temp_file(english_pdf)
                ger_temp_path = save_temp_file(german_pdf)
                english_toc = structure_toc(get_toc_text_from_pdf(eng_temp_path, page_num=toc_page_num))
                german_toc = structure_toc(get_toc_text_from_pdf(ger_temp_path, page_num=toc_page_num))

            with st.spinner("Step 4/7: Aligning ToC sections..."):
                aligned_sections = align_tocs(english_toc, german_toc)
                st.toast(f"Matched {len(aligned_sections)} ToC sections.")
                # Display matched sections in sidebar for visibility
                st.subheader("Matched Sections")
                for sec in aligned_sections:
                    st.write(f"'{sec['english']['title']}' → '{sec['german']['title']}'")

            with st.spinner("Step 5/7: Aligning content for each section..."):
                progress_bar = st.progress(0, "Aligning sections...")
                for i, section in enumerate(aligned_sections):
                    eng_sec, ger_sec = section['english'], section['german']
                    
                    eng_section_content = [item for item in full_english_content if eng_sec['start_page'] <= item['page'] <= eng_sec['end_page']]
                    ger_section_content = [item for item in full_german_content if ger_sec['start_page'] <= item['page'] <= ger_sec['end_page']]

                    if eng_section_content and ger_section_content:
                        aligned_pairs_section = align_content(eng_section_content, ger_section_content, context_window=1)
                        final_aligned_pairs.extend(aligned_pairs_section)
                    
                    progress_bar.progress((i + 1) / len(aligned_sections), f"Aligned '{eng_sec['title']}'")

            with st.spinner("Step 6/7: Saving alignment report..."):
                final_aligned_pairs.sort(key=lambda x: (x['english']['page'] if x.get('english') else float('inf')))
                alignment_report_path = output_dir / f"alignment_{base_filename}.xlsx"
                save_alignment_report(final_aligned_pairs, alignment_report_path)
                st.toast("Saved Alignment Report.")

            with st.spinner("Step 7/7: Evaluating aligned pairs for errors..."):
                st.session_state.evaluation_results = list(run_evaluation_pipeline(final_aligned_pairs))
                if st.session_state.evaluation_results:
                    eval_report_path = output_dir / f"evaluation_{base_filename}.xlsx"
                    save_evaluation_report(st.session_state.evaluation_results, eval_report_path)
                    st.toast("Saved Evaluation Report.")
            
            # Cleanup temp files
            os.remove(eng_temp_path)
            os.remove(ger_temp_path)

            st.session_state.analysis_complete = True
            st.success("Analysis pipeline finished successfully!")
            time.sleep(2)
            st.rerun()

        except Exception as e:
            st.session_state.error_message = f"An error occurred: {e}"
            st.rerun()

    # --- Download Button ---
    st.header("3. Export Results")
    if st.session_state.analysis_complete and st.session_state.evaluation_results:
        excel_data = create_excel_report_in_memory(st.session_state.evaluation_results)
        st.download_button(
            label="📥 Download Report as Excel",
            data=excel_data,
            file_name=f"Translation_Analysis_{datetime.now().strftime('%Y-%m-%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        st.markdown("_Report available after analysis._")

# --- Main Display Area ---
st.header("Evaluation Results")

if st.session_state.error_message:
    st.error(st.session_state.error_message, icon="🚨")
elif st.session_state.analysis_complete:
    if not st.session_state.evaluation_results:
        st.success("✅ Analysis complete. No significant errors were found.")
    else:
        display_results(st.session_state.evaluation_results)
else:
    st.info("Upload your PDFs and click 'Run Analysis' to begin.")
