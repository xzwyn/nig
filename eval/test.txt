import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED (THE REAL FIX):
    This parser now iterates through ALL content lists in the JSON:
    - paragraphs (which includes text, list items, footnotes, etc.)
    - tables
    - figures (which includes charts, images, and their captions)
    - sections (which includes other document segments)
    
    It extracts all of them into one single, offset-sorted list
    by slicing the main 'content' string, which is the only
    reliable source of truth for all element types.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        # The full markdown string is the single source of truth
        full_text_content = analyze_result.get('content', '')
        if not full_text_content:
             raise ValueError("Document Intelligence 'content' field is empty.")
             
        raw_paragraphs = analyze_result.get('paragraphs', [])
        raw_tables = analyze_result.get('tables', [])
        raw_figures = analyze_result.get('figures', [])
        raw_sections = analyze_result.get('sections', []) # <-- ADDED THIS
        pages = analyze_result.get('pages', [])
        
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    # --- Step 1: Create a page lookup for all items ---
    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    # --- Step 2: Extract all content items by slicing the main content string ---
    all_content: List[ContentItem] = []
    
    # Set to store offsets we've already added to prevent duplication
    # (e.g., a "section" that is also a "paragraph")
    added_offsets = set()

    # Process PARAGRAPHS (includes sectionHeading, title, listItem, footnote, etc.)
    for p in raw_paragraphs:
        role = p.get('role', 'paragraph')
        if role in config.IGNORED_ROLES or not p.get('spans'):
            continue

        span = p['spans'][0]
        offset = span['offset']
        length = span['length']
        
        if offset in added_offsets:
            continue
            
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': role, 
                'page': page_number, 
                'offset': offset
            })
            added_offsets.add(offset)

    # Process TABLES
    for table in raw_tables:
        if not table.get('spans'):
            continue
        
        span = table['spans'][0]
        offset = span['offset']
        length = span['length']

        if offset in added_offsets:
            continue

        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': 'table', 
                'page': page_number, 
                'offset': offset
            })
            added_offsets.add(offset)

    # Process FIGURES
    for fig in raw_figures:
        if not fig.get('spans'):
            continue
            
        span = fig['spans'][0]
        offset = span['offset']
        length = span['length']

        if offset in added_offsets:
            continue

        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': 'figure', 
                'page': page_number, 
                'offset': offset
            })
            added_offsets.add(offset)

    # --- NEW: Process SECTIONS ---
    for sec in raw_sections:
        # Sections often contain other elements; we only want to add them
        # if they haven't been captured as a paragraph, table, or figure.
        if not sec.get('spans'):
            continue
            
        span = sec['spans'][0]
        offset = span['offset']
        length = span['length']
        
        if offset in added_offsets:
            continue
            
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': sec.get('kind', 'section'), # 'kind' is the role for sections
                'page': page_number, 
                'offset': offset
            })
            added_offsets.add(offset)
    # --- END NEW SECTION ---

    # --- Step 3: Sort all extracted content by its character offset ---
    all_content.sort(key=lambda x: x['offset'])
    
    return all_content
