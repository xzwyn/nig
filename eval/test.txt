# test_1/src/alignment/semantic_aligner.py
from typing import List, Dict, Any, Tuple
from pathlib import Path
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import faiss

import config
from src.clients.azure_client import get_embeddings # MODIFIED: Import the centralized function

# Type Aliases for clarity
ContentItem = Dict[str, Any]
AlignedPair = Dict[str, Any]


def _build_texts_with_context(
    texts: List[str],
    content_items: List[ContentItem],
    context_window: int = 0
) -> List[str]:
    """
    Constructs a list of texts, each optionally enriched with context
    from surrounding content items.
    """
    if context_window <= 0:
        return texts

    texts_with_context = []
    for i, text in enumerate(texts):
        pre_context = "".join([f"{content_items[j]['text']} " for j in range(max(0, i - context_window), i)])
        post_context = "".join([f" {content_items[j]['text']}" for j in range(i + 1, min(len(texts), i + context_window + 1))])
        content_type = content_items[i]['type']
        page_num = content_items[i]['page']
        context_text = f"{pre_context}[SEP]{text}[SEP]{post_context} [TYPE:{content_type}] [PAGE:{page_num}]".strip()
        texts_with_context.append(context_text)
    return texts_with_context


def _calculate_margin_scores_and_matches(
    source_embeds: np.ndarray,
    target_embeds: np.ndarray,
    k: int = 4
) -> List[Tuple[int, int, float]]:
    """
    Calculates alignment scores using the margin method (CSLS-like) and returns the best 1-to-1 matches.
    This replaces the simple cosine similarity matrix and the Hungarian algorithm.
    """
    # 1. Normalize embeddings for cosine similarity calculation
    source_embeds = source_embeds / np.linalg.norm(source_embeds, axis=1, keepdims=True)
    target_embeds = target_embeds / np.linalg.norm(target_embeds, axis=1, keepdims=True)

    # 2. Find k-Nearest Neighbors (kNN) in both directions using FAISS
    # Forward: English -> German
    index_target = faiss.IndexFlatIP(target_embeds.shape[1])
    index_target.add(target_embeds)
    sim_fwd, ind_fwd = index_target.search(source_embeds, k)

    # Backward: German -> English
    index_source = faiss.IndexFlatIP(source_embeds.shape[1])
    index_source.add(source_embeds)
    sim_bwd, ind_bwd = index_source.search(target_embeds, k)

    # 3. Calculate the margin score
    mean_fwd_sim = sim_fwd.mean(axis=1)
    mean_bwd_sim = sim_bwd.mean(axis=1)

    # The margin is the ratio between the cosine distance and the average similarity of nearest neighbors.
    margin = lambda a, b: a / b
    
    scores_fwd = score_candidates(source_embeds, target_embeds, ind_fwd, mean_fwd_sim, mean_bwd_sim, margin)
    scores_bwd = score_candidates(target_embeds, source_embeds, ind_bwd, mean_bwd_sim, mean_fwd_sim, margin)

    # 4. Find the best matches based on the new scores
    fwd_best_idx = ind_fwd[np.arange(source_embeds.shape[0]), scores_fwd.argmax(axis=1)]
    bwd_best_idx = ind_bwd[np.arange(target_embeds.shape[0]), scores_bwd.argmax(axis=1)]

    # 5. Combine and enforce 1-to-1 mapping
    # This combines the best forward and backward candidates.
    potential_matches = []
    fwd_scores_max = scores_fwd.max(axis=1)
    for i in range(source_embeds.shape[0]):
        potential_matches.append((i, fwd_best_idx[i], fwd_scores_max[i]))

    bwd_scores_max = scores_bwd.max(axis=1)
    for i in range(target_embeds.shape[0]):
        potential_matches.append((bwd_best_idx[i], i, bwd_scores_max[i]))

    potential_matches.sort(key=lambda x: x[2], reverse=True)

    final_matches = []
    seen_source = set()
    seen_target = set()
    for src_idx, trg_idx, score in potential_matches:
        # Enforce strict 1-to-1 matching to prevent mispairings.
        if src_idx not in seen_source and trg_idx not in seen_target:
            final_matches.append((src_idx, trg_idx, score))
            seen_source.add(src_idx)
            seen_target.add(trg_idx)

    return final_matches
    
def score_candidates(x, y, ind, x_mean, y_mean, margin):
    """Helper function to calculate margin scores for candidates."""
    scores = np.zeros_like(ind, dtype=np.float32)
    for i in range(x.shape[0]):
        for j in range(ind.shape[1]):
            scores[i, j] = margin(np.dot(x[i], y[ind[i, j]]), (x_mean[i] + y_mean[ind[i, j]]) / 2)
    return scores


def align_content(
    english_content: List[ContentItem],
    german_content: List[ContentItem],
    context_window: int = 0,
) -> List[AlignedPair]:
    """
    Aligns content between English and German documents using margin-based scoring.
    """
    if not english_content or not german_content:
        return []

    # --- MODIFIED BLOCK START ---
    # 1. Prepare texts with context
    print("Preparing text with context for embeddings...")
    english_texts = [item['text'] for item in english_content]
    german_texts = [item['text'] for item in german_content]

    english_texts_with_context = _build_texts_with_context(english_texts, english_content, context_window)
    german_texts_with_context = _build_texts_with_context(german_texts, german_content, context_window)
    
    # 2. Get embeddings using the centralized client
    print("Generating embeddings...")
    english_embeddings = np.array(get_embeddings(english_texts_with_context), dtype='float32')
    german_embeddings = np.array(get_embeddings(german_texts_with_context), dtype='float32')
    # --- MODIFIED BLOCK END ---

    # 3. Get the best 1-to-1 matches using the new margin-based method
    print("Finding best 1-to-1 matches using margin-based scoring...")
    best_matches = _calculate_margin_scores_and_matches(english_embeddings, german_embeddings)

    semantic_matrix = cosine_similarity(english_embeddings, german_embeddings)

    # 4. Create the list of aligned pairs
    aligned_pairs: List[AlignedPair] = []
    used_english_indices = set()
    used_german_indices = set()

    for eng_idx, ger_idx, score in best_matches:
        final_similarity_score = semantic_matrix[eng_idx, ger_idx]
        
        # We apply a threshold to the margin score to decide if sentences are mutual translations.
        # The Python script uses a threshold of 1.0, which is a good starting point.
        if score >= 1.0 and final_similarity_score >= config.SIMILARITY_THRESHOLD:
            aligned_pairs.append({
                "english": english_content[eng_idx],
                "german": german_content[ger_idx],
                "similarity": float(final_similarity_score)
            })
            used_english_indices.add(eng_idx)
            used_german_indices.add(ger_idx)

    # 5. Add unmatched content
    for i, item in enumerate(english_content):
        if i not in used_english_indices:
            aligned_pairs.append({"english": item, "german": None, "similarity": 0.0})

    for i, item in enumerate(german_content):
        if i not in used_german_indices:
            aligned_pairs.append({"english": None, "german": item, "similarity": 0.0})

    # 6. Sort the final list
    aligned_pairs.sort(key=lambda x: x['english']['page'] if x.get('english') else float('inf'))
    
    return aligned_pairs


# test_1/main.py
import argparse
import time
from pathlib import Path
import os

from dotenv import load_dotenv
load_dotenv()

import config
from src.clients.doc_intelligence_client import analyze_pdf
from src.processing.json_parser import process_document_json
from src.processing.toc_parser import get_toc_text_from_pdf, structure_toc
from src.alignment.toc_aligner import align_tocs
from src.alignment.semantic_aligner import align_content
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import save_alignment_report, save_evaluation_report

def main():
    parser = argparse.ArgumentParser(
        description="Aligns and evaluates translated PDF documents based on their Table of Contents."
    )
    parser.add_argument("english_pdf", type=str, help="Path to the English PDF file (source).")
    parser.add_argument("german_pdf", type=str, help="Path to the German PDF file (translation).")
    parser.add_argument(
        "--toc-page", type=int, default=2,
        help="The page number where the Table of Contents is located (default: 2)."
    )
    parser.add_argument(
        "--context-window", type=int, default=1,
        help="Size of context window for embeddings (default: 1 for context, 0 for no context)."
    )
    parser.add_argument(
        "--evaluate", action="store_true",
        help="Run the AI evaluation pipeline after alignment."
    )
    parser.add_argument(
        "-o", "--output", type=str, help="Base name for the output report files.",
        default=None
    )
    args = parser.parse_args()

    # --- 1. Setup Paths ---
    eng_pdf_path = Path(args.english_pdf)
    ger_pdf_path = Path(args.german_pdf)

    output_dir = Path(config.OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    base_filename = args.output if args.output else f"{eng_pdf_path.stem}_{timestamp}"
    
    output_alignment_path = output_dir / f"alignment_{base_filename}.xlsx"
    output_evaluation_path = output_dir / f"evaluation_{base_filename}.xlsx"

    print("--- ToC-Based Document Alignment Pipeline Started ---")
    print(f"English Source: {eng_pdf_path}")
    print(f"German Source:  {ger_pdf_path}")
    print(f"Alignment Algorithm: ToC Section Matching + Margin-Based Content Alignment")
    print("-" * 50)

    try:
        final_aligned_pairs = []

        # Step 1: Analyze PDFs with Document Intelligence
        print("Step 1/6: Analyzing PDFs with Azure Document Intelligence...")
        with open(eng_pdf_path, "rb") as f:
            eng_json_data = analyze_pdf(f.read(), eng_pdf_path.name)
        with open(ger_pdf_path, "rb") as f:
            ger_json_data = analyze_pdf(f.read(), ger_pdf_path.name)
        print("-> Analysis complete.\n")

        # Step 2: Process full document content
        print("Step 2/6: Processing full document content...")
        full_english_content = process_document_json(eng_json_data)
        full_german_content = process_document_json(ger_json_data)
        print(f"-> Extracted {len(full_english_content)} English segments and {len(full_german_content)} German segments.\n")

        # Step 3: Extract and structure Tables of Contents
        print("Step 3/6: Extracting and structuring Tables of Contents...")
        toc_page_num = args.toc_page - 1  # Adjust for 0-based indexing
        english_toc = structure_toc(get_toc_text_from_pdf(eng_pdf_path, page_num=toc_page_num))
        german_toc = structure_toc(get_toc_text_from_pdf(ger_pdf_path, page_num=toc_page_num))
        print("-> ToC extraction complete.\n")

        # Step 4: Align ToC sections
        print("Step 4/6: Aligning ToC sections...")
        aligned_sections = align_tocs(english_toc, german_toc)
        print(f"-> Matched {len(aligned_sections)} ToC sections.\n")

        # Step 5: Align content for each section
        print("Step 5/6: Aligning content for each matched section...")
        for i, section in enumerate(aligned_sections):
            eng_sec, ger_sec = section['english'], section['german']
            print(f"  - Aligning '{eng_sec['title']}'...")
            
            eng_section_content = [item for item in full_english_content if eng_sec['start_page'] <= item['page'] <= eng_sec['end_page']]
            ger_section_content = [item for item in full_german_content if ger_sec['start_page'] <= item['page'] <= ger_sec['end_page']]

            if eng_section_content and ger_section_content:
                aligned_pairs_section = align_content(eng_section_content, ger_section_content, context_window=args.context_window)
                final_aligned_pairs.extend(aligned_pairs_section)
        print("-> Sectional alignment complete.\n")
        
        # Step 6: Save alignment report
        print("Step 6/6: Saving final alignment report...")
        final_aligned_pairs.sort(key=lambda x: (x['english']['page'] if x.get('english') else float('inf')))
        save_alignment_report(final_aligned_pairs, output_alignment_path)
        print(f"-> Alignment report saved to: {output_alignment_path.resolve()}\n")

        # Optional Step 7: Evaluate
        if args.evaluate:
            print("Bonus Step: Running AI evaluation pipeline...")
            evaluation_results = list(run_evaluation_pipeline(final_aligned_pairs))
            if evaluation_results:
                save_evaluation_report(evaluation_results, output_evaluation_path)
                print(f"-> Found {len(evaluation_results)} potential issues.")
                print(f"-> Evaluation report saved to: {output_evaluation_path.resolve()}")
            else:
                print("-> Evaluation complete. No significant errors were found.")

    except Exception as e:
        print(f"\nAn error occurred during the pipeline: {e}")
        return

    print("\n--- Pipeline Finished Successfully ---")

if __name__ == "__main__":
    main()


# test_1/src/reporting/excel_writer.py
import io
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import numpy as np

import config

AlignedPair = Dict[str, Any]
EvaluationFinding = Dict[str, Any]
ContentItem = Dict[str, Any]

def save_alignment_report(aligned_data: List[AlignedPair], filepath: Path) -> None:
    """Saves the document alignment data to an Excel file."""
    if not aligned_data:
        print("Warning: No aligned data to save to Excel.")
        return
    report_data = []
    for pair in aligned_data:
        eng_item = pair.get('english')
        ger_item = pair.get('german')
        report_data.append({
            "English": eng_item.get('text', '') if eng_item else "--- OMITTED ---",
            "German": ger_item.get('text', '') if ger_item else "--- ADDED ---",
            "Similarity": f"{pair.get('similarity', 0.0):.4f}",
            "Type": (eng_item.get('type') if eng_item else ger_item.get('type', 'N/A')),
            "English Page": (eng_item.get('page') if eng_item else 'N/A'),
            "German Page": (ger_item.get('page') if ger_item else 'N/A')
        })
    df = pd.DataFrame(report_data)
    try:
        df.to_excel(filepath, index=False, engine='openpyxl')
    except Exception as e:
        print(f"Error: Could not write alignment report to '{filepath}'. Reason: {e}")


def save_evaluation_report(evaluation_results: List[EvaluationFinding], filepath: Path) -> None:
    """Saves the AI evaluation findings to a separate Excel report."""
    if not evaluation_results:
        print("No evaluation findings to save.")
        return
    evaluation_results.sort(key=lambda x: x.get('page', 0))
    df = pd.DataFrame(evaluation_results)
    desired_columns = [
        "page", "type", "suggestion", "english_text", "german_text",
        "original_phrase", "translated_phrase"
    ]
    final_columns = [col for col in desired_columns if col in df.columns]
    df = df[final_columns]
    try:
        df.to_excel(filepath, index=False, sheet_name='Evaluation_Findings')
    except Exception as e:
        print(f"Error: Could not write evaluation report to '{filepath}'. Reason: {e}")


# DELETED: The `save_calculation_report` function was here and has been removed.


def create_excel_report_in_memory(evaluation_results: List[EvaluationFinding]) -> bytes:
    """
    Creates the evaluation Excel report in memory and returns it as bytes.
    """
    if not evaluation_results:
        return b'' # Return empty bytes if there's nothing to report

    evaluation_results.sort(key=lambda x: x.get('page', 0))
    df = pd.DataFrame(evaluation_results)

    # Define the desired column order for the final report
    desired_columns = [
        "page", "type", "suggestion", "english_text", "german_text",
        "original_phrase", "translated_phrase"
    ]
    # Filter the dataframe to only include existing columns in the desired order
    final_columns = [col for col in desired_columns if col in df.columns]
    df = df[final_columns]

    # Use an in-memory buffer to save the Excel file
    output_buffer = io.BytesIO()
    with pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Evaluation_Findings')

    # Retrieve the byte data from the buffer
    return output_buffer.getvalue()



# test_1/main.py
import argparse
import time
from pathlib import Path
import os

from dotenv import load_dotenv
load_dotenv()

import config
from src.clients.doc_intelligence_client import analyze_pdf
from src.processing.json_parser import process_document_json
from src.processing.toc_parser import get_toc_text_from_pdf, structure_toc
from src.alignment.toc_aligner import align_tocs
from src.alignment.semantic_aligner import align_content
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import save_alignment_report, save_evaluation_report

def main():
    # --- MODIFIED BLOCK START ---
    # The parser has been updated to reflect the current functionality.
    parser = argparse.ArgumentParser(
        description="Aligns and evaluates translated PDF documents based on their Table of Contents." # MODIFIED: Description is now accurate.
    )
    parser.add_argument("english_pdf", type=str, help="Path to the English PDF file (source).")
    parser.add_argument("german_pdf", type=str, help="Path to the German PDF file (translation).")
    parser.add_argument(
        "--toc-page", type=int, default=2,
        help="The page number where the Table of Contents is located (default: 2)."
    )
    parser.add_argument(
        "--context-window", type=int, default=1,
        help="Size of context window for embeddings (default: 1 for context, 0 for no context)."
    )
    parser.add_argument(
        "--evaluate", action="store_true",
        help="Run the AI evaluation pipeline after alignment."
    )
    parser.add_argument(
        "-o", "--output", type=str, help="Base name for the output report files.",
        default=None
    )
    # DELETED: The `--debug-report` argument has been removed.
    # --- MODIFIED BLOCK END ---
    args = parser.parse_args()

    # --- 1. Setup Paths ---
    eng_pdf_path = Path(args.english_pdf)
    ger_pdf_path = Path(args.german_pdf)

    output_dir = Path(config.OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    base_filename = args.output if args.output else f"{eng_pdf_path.stem}_{timestamp}"
    
    output_alignment_path = output_dir / f"alignment_{base_filename}.xlsx"
    output_evaluation_path = output_dir / f"evaluation_{base_filename}.xlsx"

    print("--- ToC-Based Document Alignment Pipeline Started ---")
    print(f"English Source: {eng_pdf_path}")
    print(f"German Source:  {ger_pdf_path}")
    # MODIFIED: Print statement accurately describes the method.
    print(f"Alignment Algorithm: ToC Section Matching + Margin-Based Content Alignment")
    print("-" * 50)

    try:
        final_aligned_pairs = []

        # Step 1: Analyze PDFs with Document Intelligence
        print("Step 1/6: Analyzing PDFs with Azure Document Intelligence...")
        with open(eng_pdf_path, "rb") as f:
            eng_json_data = analyze_pdf(f.read(), eng_pdf_path.name)
        with open(ger_pdf_path, "rb") as f:
            ger_json_data = analyze_pdf(f.read(), ger_pdf_path.name)
        print("-> Analysis complete.\n")

        # Step 2: Process full document content
        print("Step 2/6: Processing full document content...")
        full_english_content = process_document_json(eng_json_data)
        full_german_content = process_document_json(ger_json_data)
        print(f"-> Extracted {len(full_english_content)} English segments and {len(full_german_content)} German segments.\n")

        # Step 3: Extract and structure Tables of Contents
        print("Step 3/6: Extracting and structuring Tables of Contents...")
        toc_page_num = args.toc_page - 1  # Adjust for 0-based indexing
        english_toc = structure_toc(get_toc_text_from_pdf(eng_pdf_path, page_num=toc_page_num))
        german_toc = structure_toc(get_toc_text_from_pdf(ger_pdf_path, page_num=toc_page_num))
        print("-> ToC extraction complete.\n")

        # Step 4: Align ToC sections
        print("Step 4/6: Aligning ToC sections...")
        aligned_sections = align_tocs(english_toc, german_toc)
        print(f"-> Matched {len(aligned_sections)} ToC sections.\n")

        # Step 5: Align content for each section
        print("Step 5/6: Aligning content for each matched section...")
        for i, section in enumerate(aligned_sections):
            eng_sec, ger_sec = section['english'], section['german']
            print(f"  - Aligning '{eng_sec['title']}'...")
            
            eng_section_content = [item for item in full_english_content if eng_sec['start_page'] <= item['page'] <= eng_sec['end_page']]
            ger_section_content = [item for item in full_german_content if ger_sec['start_page'] <= item['page'] <= ger_sec['end_page']]

            if eng_section_content and ger_section_content:
                aligned_pairs_section = align_content(eng_section_content, ger_section_content, context_window=args.context_window)
                final_aligned_pairs.extend(aligned_pairs_section)
        print("-> Sectional alignment complete.\n")
        
        # Step 6: Save alignment report
        print("Step 6/6: Saving final alignment report...")
        final_aligned_pairs.sort(key=lambda x: (x['english']['page'] if x.get('english') else float('inf')))
        save_alignment_report(final_aligned_pairs, output_alignment_path)
        print(f"-> Alignment report saved to: {output_alignment_path.resolve()}\n")

        # Optional Step 7: Evaluate
        if args.evaluate:
            print("Bonus Step: Running AI evaluation pipeline...")
            evaluation_results = list(run_evaluation_pipeline(final_aligned_pairs))
            if evaluation_results:
                save_evaluation_report(evaluation_results, output_evaluation_path)
                print(f"-> Found {len(evaluation_results)} potential issues.")
                print(f"-> Evaluation report saved to: {output_evaluation_path.resolve()}")
            else:
                print("-> Evaluation complete. No significant errors were found.")

    except Exception as e:
        print(f"\nAn error occurred during the pipeline: {e}")
        return

    print("\n--- Pipeline Finished Successfully ---")

if __name__ == "__main__":
    main()
