import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED (THE REAL FIX):
    This parser now iterates through ALL content lists in the JSON:
    - paragraphs (which includes text, list items, footnotes, etc.)
    - tables
    - figures (which includes charts, images, and their captions)
    
    It extracts all of them into one single, offset-sorted list.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        # The full markdown string is the single source of truth
        full_text_content = analyze_result.get('content', '')
        if not full_text_content:
             raise ValueError("Document Intelligence 'content' field is empty.")
             
        raw_paragraphs = analyze_result.get('paragraphs', [])
        raw_tables = analyze_result.get('tables', [])
        raw_figures = analyze_result.get('figures', []) # <-- ADDED THIS
        pages = analyze_result.get('pages', [])
        
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    # --- Step 1: Create a page lookup for all items ---
    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    # --- Step 2: Extract all content items by slicing the main content string ---
    all_content: List[ContentItem] = []

    # Process PARAGRAPHS (includes sectionHeading, title, listItem, footnote, etc.)
    for p in raw_paragraphs:
        role = p.get('role', 'paragraph')
        if role in config.IGNORED_ROLES or not p.get('spans'):
            continue

        span = p['spans'][0]
        offset = span['offset']
        length = span['length']
        
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': role, 
                'page': page_number, 
                'offset': offset
            })

    # Process TABLES
    for table in raw_tables:
        if not table.get('spans'):
            continue
        
        span = table['spans'][0]
        offset = span['offset']
        length = span['length']

        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': 'table', 
                'page': page_number, 
                'offset': offset
            })

    # --- NEW: Process FIGURES ---
    for fig in raw_figures:
        if not fig.get('spans'):
            continue
            
        span = fig['spans'][0]
        offset = span['offset']
        length = span['length']

        # The 'content' of a figure in markdown is its caption.
        # This is perfect, as we want to align the captions.
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        
        if text:
            all_content.append({
                'text': text, 
                'type': 'figure', 
                'page': page_number, 
                'offset': offset
            })
    # --- END NEW SECTION ---

    # --- Step 3: Sort all extracted content by its character offset ---
    all_content.sort(key=lambda x: x['offset'])
    
    # We return the raw, sorted list of ALL items.
    return all_content
