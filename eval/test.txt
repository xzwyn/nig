# test_1/src/alignment/semantic_aligner.py
from typing import List, Dict, Any, Tuple
from pathlib import Path
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from openai import AzureOpenAI
from tqdm import tqdm
import faiss  # New import for efficient nearest neighbor search

import config

# Type Aliases for clarity
ContentItem = Dict[str, Any]
AlignedPair = Dict[str, Any]

# A reusable client instance
_client = None

def _get_azure_client() -> AzureOpenAI:
    """Initializes and returns a reusable AzureOpenAI client."""
    global _client
    if _client is None:
        print("Initializing Azure OpenAI client...")
        if not all([config.AZURE_EMBEDDING_ENDPOINT, config.AZURE_EMBEDDING_API_KEY]):
            raise ValueError("Azure credentials (endpoint, key) are not set in the config/.env file.")

        _client = AzureOpenAI(
            api_version=config.AZURE_API_VERSION,
            azure_endpoint=config.AZURE_EMBEDDING_ENDPOINT,
            api_key=config.AZURE_EMBEDDING_API_KEY,
        )
    return _client

def _get_embeddings_in_batches(
    texts: List[str],
    content_items: List[ContentItem],
    client: AzureOpenAI,
    batch_size: int = 16,
    context_window: int = 0
) -> np.ndarray:
    """
    Generates embeddings by sending texts to the Azure API in batches.
    Optionally includes context from surrounding segments.
    """
    if context_window > 0:
        texts_with_context = []
        for i, text in enumerate(texts):
            pre_context = "".join([f"{content_items[j]['text']} " for j in range(max(0, i - context_window), i)])
            post_context = "".join([f" {content_items[j]['text']}" for j in range(i + 1, min(len(texts), i + context_window + 1))])
            content_type = content_items[i]['type']
            page_num = content_items[i]['page']
            context_text = f"{pre_context}[SEP]{text}[SEP]{post_context} [TYPE:{content_type}] [PAGE:{page_num}]".strip()
            texts_with_context.append(context_text)
        texts_to_embed = texts_with_context
    else:
        texts_to_embed = texts

    all_embeddings = []
    for i in tqdm(range(0, len(texts_to_embed), batch_size), desc="Generating Embeddings"):
        batch = texts_to_embed[i:i + batch_size]
        try:
            response = client.embeddings.create(input=batch, model=config.AZURE_EMBEDDING_DEPLOYMENT_NAME)
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
        except Exception as e:
            print(f"An error occurred while processing a batch: {e}")
            all_embeddings.extend([[0.0] * 3072] * len(batch))

    return np.array(all_embeddings, dtype='float32')


def _calculate_margin_scores_and_matches(
    source_embeds: np.ndarray,
    target_embeds: np.ndarray,
    k: int = 4
) -> List[Tuple[int, int, float]]:
    """
    Calculates alignment scores using the margin method (CSLS-like) and returns the best 1-to-1 matches.
    This replaces the simple cosine similarity matrix and the Hungarian algorithm.
    """
    # 1. Normalize embeddings for cosine similarity calculation
    source_embeds = source_embeds / np.linalg.norm(source_embeds, axis=1, keepdims=True)
    target_embeds = target_embeds / np.linalg.norm(target_embeds, axis=1, keepdims=True)

    # 2. Find k-Nearest Neighbors (kNN) in both directions using FAISS
    # Forward: English -> German
    index_target = faiss.IndexFlatIP(target_embeds.shape[1])
    index_target.add(target_embeds)
    sim_fwd, ind_fwd = index_target.search(source_embeds, k)

    # Backward: German -> English
    index_source = faiss.IndexFlatIP(source_embeds.shape[1])
    index_source.add(source_embeds)
    sim_bwd, ind_bwd = index_source.search(target_embeds, k)

    # 3. Calculate the margin score
    mean_fwd_sim = sim_fwd.mean(axis=1)
    mean_bwd_sim = sim_bwd.mean(axis=1)

    # [cite_start]The margin is the ratio between the cosine distance and the average similarity of nearest neighbors. [cite: 665]
    margin = lambda a, b: a / b
    
    scores_fwd = score_candidates(source_embeds, target_embeds, ind_fwd, mean_fwd_sim, mean_bwd_sim, margin)
    scores_bwd = score_candidates(target_embeds, source_embeds, ind_bwd, mean_bwd_sim, mean_fwd_sim, margin)

    # 4. Find the best matches based on the new scores
    fwd_best_idx = ind_fwd[np.arange(source_embeds.shape[0]), scores_fwd.argmax(axis=1)]
    bwd_best_idx = ind_bwd[np.arange(target_embeds.shape[0]), scores_bwd.argmax(axis=1)]

    # 5. Combine and enforce 1-to-1 mapping
    # [cite_start]This combines the best forward and backward candidates. [cite: 671]
    potential_matches = []
    fwd_scores_max = scores_fwd.max(axis=1)
    for i in range(source_embeds.shape[0]):
        potential_matches.append((i, fwd_best_idx[i], fwd_scores_max[i]))

    bwd_scores_max = scores_bwd.max(axis=1)
    for i in range(target_embeds.shape[0]):
        potential_matches.append((bwd_best_idx[i], i, bwd_scores_max[i]))

    potential_matches.sort(key=lambda x: x[2], reverse=True)

    final_matches = []
    seen_source = set()
    seen_target = set()
    for src_idx, trg_idx, score in potential_matches:
        # [cite_start]Enforce strict 1-to-1 matching to prevent mispairings. [cite: 671]
        if src_idx not in seen_source and trg_idx not in seen_target:
            final_matches.append((src_idx, trg_idx, score))
            seen_source.add(src_idx)
            seen_target.add(trg_idx)

    return final_matches
    
def score_candidates(x, y, ind, x_mean, y_mean, margin):
    """Helper function to calculate margin scores for candidates."""
    scores = np.zeros_like(ind, dtype=np.float32)
    for i in range(x.shape[0]):
        for j in range(ind.shape[1]):
            scores[i, j] = margin(np.dot(x[i], y[ind[i, j]]), (x_mean[i] + y_mean[ind[i, j]]) / 2)
    return scores


def align_content(
    english_content: List[ContentItem],
    german_content: List[ContentItem],
    context_window: int = 0,
    generate_debug_report: bool = False,
    debug_report_path: Path = None
) -> List[AlignedPair]:
    """
    Aligns content between English and German documents using margin-based scoring.
    """
    if not english_content or not german_content:
        return []

    client = _get_azure_client()

    # 1. Get embeddings
    english_embeddings = _get_embeddings_in_batches(
        [item['text'] for item in english_content], english_content, client, context_window=context_window
    )
    german_embeddings = _get_embeddings_in_batches(
        [item['text'] for item in german_content], german_content, client, context_window=context_window
    )

    # 2. Get the best 1-to-1 matches using the new margin-based method
    print("Finding best 1-to-1 matches using margin-based scoring...")
    best_matches = _calculate_margin_scores_and_matches(english_embeddings, german_embeddings)

    semantic_matrix = cosine_similarity(english_embeddings, german_embeddings)

    # 3. Create the list of aligned pairs
    aligned_pairs: List[AlignedPair] = []
    used_english_indices = set()
    used_german_indices = set()

    for eng_idx, ger_idx, score in best_matches:
        final_similarity_score = semantic_matrix[eng_idx, ger_idx]
        
        # [cite_start]We apply a threshold to the margin score to decide if sentences are mutual translations. [cite: 672]
        # The Python script uses a threshold of 1.0, which is a good starting point.
        if score >= 1.0 and final_similarity_score >= config.SIMILARITY_THRESHOLD:
            aligned_pairs.append({
                "english": english_content[eng_idx],
                "german": german_content[ger_idx],
                "similarity": float(final_similarity_score)
            })
            used_english_indices.add(eng_idx)
            used_german_indices.add(ger_idx)

    # 4. Add unmatched content
    for i, item in enumerate(english_content):
        if i not in used_english_indices:
            aligned_pairs.append({"english": item, "german": None, "similarity": 0.0})

    for i, item in enumerate(german_content):
        if i not in used_german_indices:
            aligned_pairs.append({"english": None, "german": item, "similarity": 0.0})

    # 5. Sort the final list
    aligned_pairs.sort(key=lambda x: x['english']['page'] if x.get('english') else float('inf'))
    
    if generate_debug_report and debug_report_path:
        print("WARNING: Debug report generation is not supported with the new margin-based alignment method.")

    return aligned_pairs
