// config.py
import os
from dotenv import load_dotenv

load_dotenv()

AZURE_EMBEDDING_ENDPOINT = os.getenv("AZURE_EMBEDDING_ENDPOINT")
AZURE_EMBEDDING_API_KEY = os.getenv("AZURE_EMBEDDING_API_KEY")
AZURE_EMBEDDING_DEPLOYMENT_NAME = os.getenv("AZURE_EMBEDDING_DEPLOYMENT_NAME")
AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-02-01")

IGNORED_ROLES = {'pageHeader', 'pageFooter', 'pageNumber'}
STRUCTURAL_ROLES = {'title', 'sectionHeading'}

W_SEMANTIC = 1  # Weight for semantic similarity (cosine score)
W_TYPE = 0      # Weight for matching content types (e.g., table vs. table)
W_PROXIMITY = 0 # Weight for relative position in the document

TYPE_MATCH_BONUS = 0.1
TYPE_MISMATCH_PENALTY = -0.2

SIMILARITY_THRESHOLD = -1.0

MARGIN_SCORE_THRESHOLD = -1.0


INPUT_DIR: str = "input"
OUTPUT_DIR: str = "output"

// src/alignment/heading_aligner.py
import json
from typing import List, Dict, Any, Tuple
import numpy as np
from scipy.optimize import linear_sum_assignment
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Import clients and report writers
from src.clients.azure_client import get_embeddings, chat
from src.reporting.excel_writer import (
    save_raw_headings_report, 
    save_matched_headings_report,
    save_validated_headings_report
)

# Type Alias for clarity
ContentItem = Dict[str, Any]
HeadingPair = Tuple[ContentItem, ContentItem]

def _validate_heading_pair(eng_heading: str, ger_heading: str) -> bool:
    """
    Uses a 'liberal' LLM prompt to check if two headings are plausible translations.
    """
    prompt = f"""
## ROLE
You are a fast, liberal document analyst. Your goal is to check if two section headings *could* plausibly refer to the same section, even if they aren't exact translations.

## TASK
Compare the English and German headings. Respond with a single JSON object containing one key, "match", with a value of "Yes" or "No".

## RULES
- Respond "Yes" if they are clear translations (e.g., "Board Report" -> "Bericht des Vorstands").
- Respond "Yes" if they refer to the same concept (e.g., "Corporate Governance" -> "ErklÃ¤rung zur UnternehmensfÃ¼hrung").
- Respond "Yes" if one is a summary of the other (e.g., "Notes to the Financial Statements" -> "Anhang").
- Respond "No" *only* if they are clearly about different topics (e.g., "Risk Report" -> "Human Resources").

## HEADINGS
<English>
{eng_heading}
</English>

<German>
{ger_heading}
</German>

## RESPONSE (JSON ONLY)
{{ "match": "Yes" | "No" }}
"""
    try:
        content = chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        ).strip()
        
        j0, j1 = content.find("{"), content.rfind("}") + 1
        result = json.loads(content[j0:j1])
        return result.get("match", "No").lower() == "yes"
        
    except Exception as e:
        print(f"Warning: Heading validation LLM call failed. Defaulting to 'No'. Error: {e}")
        return False

def match_and_validate_headings(
    english_content: List[ContentItem],
    german_content: List[ContentItem],
    eng_start_page: int,
    ger_start_page: int,
    output_dir: Any, # Expects a Path object
    base_filename: str
) -> List[HeadingPair]:
    """
    Main orchestration function for Phase 1:
    1. Filters for headings after the start page.
    2. Gets embeddings and matches them using the Hungarian algorithm.
    3. Validates all matches using an LLM.
    4. Saves debug reports at each step.
    """
    
    # 1. Filter for headings *after* the specified start page
    print(f"Filtering headings. EN start page: {eng_start_page}, DE start page: {ger_start_page}")
    eng_headings = [
        item for item in english_content 
        if item['type'] in {'title', 'sectionHeading'} and item['page'] >= eng_start_page
    ]
    ger_headings = [
        item for item in german_content 
        if item['type'] in {'title', 'sectionHeading'} and item['page'] >= ger_start_page
    ]
    
    if not eng_headings or not ger_headings:
        print("Error: No headings found after the start page in one or both documents.")
        return []

    # Save debug report 1: Raw Headings
    save_raw_headings_report(eng_headings, ger_headings, output_dir / f"headings_0_raw_{base_filename}.xlsx")

    # 2. Get Embeddings
    print("Getting embeddings for headings...")
    eng_heading_texts = [h['text'] for h in eng_headings]
    ger_heading_texts = [h['text'] for h in ger_headings]
    
    eng_embeddings = np.array(get_embeddings(eng_heading_texts))
    ger_embeddings = np.array(get_embeddings(ger_heading_texts))
    
    # 3. Calculate Cost Matrix and run Hungarian Algorithm
    print("Calculating similarity and running Hungarian algorithm...")
    similarity_matrix = cosine_similarity(eng_embeddings, ger_embeddings)
    cost_matrix = -similarity_matrix  # We want to maximize similarity, so minimize negative similarity
    
    row_indices, col_indices = linear_sum_assignment(cost_matrix)
    
    # 4. Create initial matched list and save debug report 2
    matched_pairs = []
    unmatched_eng = list(range(len(eng_headings)))
    unmatched_ger = list(range(len(ger_headings)))

    for eng_idx, ger_idx in zip(row_indices, col_indices):
        similarity = similarity_matrix[eng_idx, ger_idx]
        matched_pairs.append({
            "english": eng_headings[eng_idx],
            "german": ger_headings[ger_idx],
            "similarity": similarity
        })
        if eng_idx in unmatched_eng:
            unmatched_eng.remove(eng_idx)
        if ger_idx in unmatched_ger:
            unmatched_ger.remove(ger_idx)
            
    # Add unmatched items for the report
    for eng_idx in unmatched_eng:
        matched_pairs.append({"english": eng_headings[eng_idx], "german": None, "similarity": 0.0})
    for ger_idx in unmatched_ger:
        matched_pairs.append({"english": None, "german": ger_headings[ger_idx], "similarity": 0.0})

    save_matched_headings_report(matched_pairs, output_dir / f"headings_1_matched_{base_filename}.xlsx")

    # 5. Validate pairs using LLM
    print(f"Validating {len(row_indices)} matched pairs with LLM...")
    validated_pairs: List[HeadingPair] = []
    
    for pair in tqdm(matched_pairs, desc="Validating Headings"):
        eng_item = pair.get('english')
        ger_item = pair.get('german')
        
        # Only validate actual pairs
        if eng_item and ger_item:
            is_valid = _validate_heading_pair(eng_item['text'], ger_item['text'])
            pair['is_valid'] = is_valid
            if is_valid:
                # Add the tuple of full ContentItems
                validated_pairs.append((eng_item, ger_item))
        else:
            pair['is_valid'] = False

    # Save debug report 3: Validated Headings
    save_validated_headings_report(matched_pairs, output_dir / f"headings_2_validated_{base_filename}.xlsx")

    print(f"Found {len(validated_pairs)} validated heading pairs.")
    
    # Sort by English document order before returning
    validated_pairs.sort(key=lambda pair: pair[0]['offset'])
    
    return validated_pairs

// src/clients/azure_client.py
import os
from typing import List, Dict, Any, Optional
from openai import AzureOpenAI
from dotenv import load_dotenv
load_dotenv()

_chat_client: Optional[AzureOpenAI] = None
_embedding_client: Optional[AzureOpenAI] = None

_cfg = {
    "chat_endpoint": None,
    "chat_api_key": None,
    "chat_api_version": None,
    "chat_deployment": None,
    "embedding_endpoint": None,
    "embedding_api_key": None,
    "embedding_api_version": None,
    "embedding_deployment": None,
}

def _load_env():
    # Chat configuration
    _cfg["chat_endpoint"] = os.getenv("AZURE_OPENAI_ENDPOINT")
    _cfg["chat_api_key"] = os.getenv("AZURE_OPENAI_API_KEY")
    _cfg["chat_api_version"] = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01")
    _cfg["chat_deployment"] = os.getenv("AZURE_OPENAI_DEPLOYMENT")

    # Embedding configuration
    _cfg["embedding_endpoint"] = os.getenv("AZURE_EMBEDDING_ENDPOINT")
    _cfg["embedding_api_key"] = os.getenv("AZURE_EMBEDDING_API_KEY")
    _cfg["embedding_api_version"] = os.getenv("AZURE_API_VERSION", "2024-02-01")
    _cfg["embedding_deployment"] = os.getenv("AZURE_EMBEDDING_DEPLOYMENT_NAME")

def _get_chat_client() -> AzureOpenAI:
    global _chat_client
    if _chat_client is not None:
        return _chat_client

    _load_env()
    if not _cfg["chat_endpoint"] or not _cfg["chat_api_key"] or not _cfg["chat_deployment"]:
        raise RuntimeError(
            "Azure OpenAI chat client is not configured. "
            "Set AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, and AZURE_OPENAI_DEPLOYMENT in your .env file."
        )

    _chat_client = AzureOpenAI(
        azure_endpoint=_cfg["chat_endpoint"],
        api_key=_cfg["chat_api_key"],
        api_version=_cfg["chat_api_version"],
    )
    return _chat_client

def _get_embedding_client() -> AzureOpenAI:
    global _embedding_client
    if _embedding_client is not None:
        return _embedding_client

    _load_env()
    if not _cfg["embedding_endpoint"] or not _cfg["embedding_api_key"] or not _cfg["embedding_deployment"]:
        raise RuntimeError(
            "Azure OpenAI embedding client is not configured. "
            "Set AZURE_EMBEDDING_ENDPOINT, AZURE_EMBEDDING_API_KEY, and AZURE_EMBEDDING_DEPLOYMENT_NAME in your .env file."
        )

    _embedding_client = AzureOpenAI(
        azure_endpoint=_cfg["embedding_endpoint"],
        api_key=_cfg["embedding_api_key"],
        api_version=_cfg["embedding_api_version"] or _cfg["chat_api_version"],
    )
    return _embedding_client

def chat(messages: List[Dict[str, Any]], temperature: float = 0.1, model: Optional[str] = None) -> str:
    client = _get_chat_client()
    deployment = model or _cfg["chat_deployment"]

    resp = client.chat.completions.create(
        model=deployment,
        messages=messages,
        temperature=temperature,
    )
    return resp.choices[0].message.content or ""

def get_embeddings(texts: List[str], model: Optional[str]=None) -> List[List[float]]:
    client = _get_embedding_client()
    deployment = model or _cfg['embedding_deployment']

    if not deployment:
        raise ValueError("No embedding deployment specified. Please set AZURE_EMBEDDING_DEPLOYMENT_NAME in your .env file.")

    response = client.embeddings.create(
        input=texts,
        model=deployment
    )
    return [item.embedding for item in response.data]

// src/clients/doc_intelligence_client.py
import os
import json
from datetime import datetime
from pathlib import Path

from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient

import config

def analyze_pdf(pdf_bytes: bytes, original_filename: str) -> dict:
    endpoint = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
    key = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")

    if not endpoint or not key:
        raise ValueError(
            "Azure Document Intelligence credentials are not configured. "
            "Set AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT and AZURE_DOCUMENT_INTELLIGENCE_KEY in your .env file."
        )

    print(f"Connecting to Document Intelligence service for '{original_filename}'...")
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))

    poller = client.begin_analyze_document(
        model_id="prebuilt-layout", 
        body=pdf_bytes, 
        content_type="application/pdf",
        output_content_format="markdown"  
    )
    
    result = poller.result()
    print("-> Analysis complete.")

    # --- Setup output paths ---
    output_dir = Path(config.OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = Path(original_filename).stem

    if result.content:
        output_md_filename = f"doc_intelligence_raw_{base_name}_{timestamp}.md"
        output_md_path = output_dir / output_md_filename
        try:
            with open(output_md_path, 'w', encoding='utf-8') as f:
                f.write(result.content)
            print(f"-> Saved raw Markdown output to '{output_md_path}'")
        except Exception as e:
            print(f"Warning: Could not save raw Markdown output. Reason: {e}")

    try:
        ar_dict = result.as_dict()  # newer SDKs
    except AttributeError:
        try:
            ar_dict = result.to_dict()  # older SDKs
        except AttributeError:
            # Manual fallback
            ar_dict = {
                "content": getattr(result, "content", ""),
                "pages": [p.as_dict() if hasattr(p, "as_dict") else (p.to_dict() if hasattr(p, "to_dict") else {}) for p in getattr(result, "pages", [])],
                "paragraphs": [x.as_dict() if hasattr(x, "as_dict") else (x.to_dict() if hasattr(x, "to_dict") else {}) for x in getattr(result, "paragraphs", [])],
                "tables": [t.as_dict() if hasattr(t, "as_dict") else (t.to_dict() if hasattr(t, "to_dict") else {}) for t in getattr(result, "tables", [])],
                "styles": [s.as_dict() if hasattr(s, "as_dict") else (s.to_dict() if hasattr(s, "to_dict") else {}) for s in getattr(result, "styles", [])],
            }

    result_dict = {"analyzeResult": ar_dict}

    # Save JSON for reference
    output_filename = f"doc_intelligence_{base_name}_{timestamp}.json"
    output_path = output_dir / output_filename

    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=4)
        print(f"-> Saved Document Intelligence JSON to '{output_path}'")
    except Exception as e:
        print(f"Warning: Could not save JSON output for '{original_filename}'. Reason: {e}")

    return result_dict

// src/evaluation/evaluators.py
import json
from src.clients.azure_client import chat  

def evaluate_translation_pair(eng_text: str, ger_text: str, model_name=None):
    prompt = f"""
## ROLE
You are the Primary Translation Auditor for ENâ†’DE corporate reports.

## TASK
Identify only the two fatal error categories below and output ONE JSON object.

## ERROR TYPES YOU MAY REPORT
1. Mistranslation
   â€¢ Wrong numeric value (digits, words, units, decimals, percentages)
   â€¢ Polarity flip / negation error (e.g., required â†” not required)
   â€¢ Change of actor or agency (who did/decided/informed whom)

2. Omission
   The English text states a concrete count (â€œtwoâ€, â€œthreeâ€, â€œbothâ€, â€œeitherâ€) or lists specific items, and at least one required element is missing in German.

Do not flag: stylistic differences, safe synonyms, acceptable German report titles (â€œNichtfinanzielle ErklÃ¤rungâ€, â€œErklÃ¤rung zur UnternehmensfÃ¼hrungâ€ etc.), benign reordering, or tense/voice changes that preserve actor and meaning.

If no fatal error is found, return error_type "None".

If multiple fatal errors exist, choose the most impactful; if tied, prefer "Mistranslation".

## JSON OUTPUT SCHEMA
json {{ "error_type" : "Mistranslation" | "Omission" | "None", "original_phrase" : "", "translated_phrase": "", "explanation" : "<â‰¤40 words>", "suggestion" : "" }}

## POSITIVE EXAMPLES
1 Â· Mistranslation (number)
EN â€œRevenue increased by 2.3 million.â€
DE â€œDer Umsatz stieg um 2,8 Millionen.â€
â†’ error_type â€œMistranslationâ€, original â€œ2.3 millionâ€, translated â€œ2,8 Millionenâ€

2 Â· Mistranslation (polarity)
EN â€œThe audit is not required.â€
DE â€œDie PrÃ¼fung ist erforderlich.â€
â†’ error_type â€œMistranslationâ€, original â€œnot requiredâ€, translated â€œerforderlichâ€

3 Â· Mistranslation (actor/agency)
EN â€œThe company was notified by the regulator.â€
DE â€œDas Unternehmen informierte die AufsichtsbehÃ¶rde.â€
â†’ error_type â€œMistranslationâ€, original â€œwas notified by the regulatorâ€, translated â€œinformierte die AufsichtsbehÃ¶rdeâ€

4 Â· Omission (enumeration/count)
EN â€œBoth measures will apply: cost cap and hiring freeze.â€
DE â€œEs gilt die Einstellungsstop.â€
â†’ error_type â€œOmissionâ€, original â€œcost capâ€, translated â€œâ€

5 Â· None (acceptable variation)
EN â€œThe report is comprehensive.â€
DE â€œDer Bericht ist umfassend.â€
â†’ error_type â€œNoneâ€

## TEXTS TO AUDIT
<Original English>
{eng_text}
</Original English>

<German Translation>
{ger_text}
</German Translation>

## YOUR RESPONSE
Return the JSON object onlyâ€”no extra text, no markdown.

## NOTES
- Compare all numbers, signs, and units (%, bps, million/Mio., billion/Mrd.).
- Treat passive/active voice as fine unless the responsible actor changes.
- For omissions, ensure every counted or listed element appears in German.
- Keep â€œexplanationâ€ concise; â€œsuggestionâ€ should minimally correct the German (or note the missing item).
"""
    try:
        content = chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            model=model_name,
        ).strip()

        j0, j1 = content.find("{"), content.rfind("}") + 1
        if j0 != -1 and j1 != -1:
            return json.loads(content[j0:j1])
        return {"error_type": "System Error",
                "explanation": "No JSON object in LLM reply."}
    except Exception as exc:
        print(f"evaluate_translation_pair â†’ {exc}")
        return {"error_type": "System Error", "explanation": str(exc)}

def check_context_mismatch(eng_text: str, ger_text: str, model_name: str = None):
    prompt = f"""
ROLE: Narrative-Integrity Analyst

Goal: Decide if the German text tells a **different story** from the
English.  â€œDifferentâ€ means a change in
â€¢ WHO does WHAT to WHOM
â€¢ factual outcome or direction of action
â€¢ polarity (e.g. â€œcomprehensiveâ€ â†” â€œunvollstÃ¤ndigâ€)

Ignore style, word order, or minor re-phrasing.

Respond with JSON:

{{
  "context_match": "Yes" | "No",
  "explanation":  "<one concise sentence>"
}}

Examples
--------
1) Role reversal (should be No)
EN  Further, the committee *was informed* by the Board â€¦
DE  DarÃ¼ber hinaus *leitete der Ausschuss eine Untersuchung ein* â€¦
â†’ roles flipped â‡’ "No"

2) Identical meaning (Yes)
EN  Declaration of Conformity with the German Corporate Governance Code
DE  EntsprechenserklÃ¤rung zum Deutschen Corporate Governance Kodex
â†’ "Yes"

Analyse the following text pair and respond with the JSON only.

<Original_English>
{eng_text}
</Original_English>

<German_Translation>
{ger_text}
</German_Translation>
"""
    try:
        content = chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            model=model_name,
        ).strip()

        j0, j1 = content.find("{"), content.rfind("}") + 1
        return json.loads(content[j0:j1])
    except Exception as exc:
        return {"context_match": "Error", "explanation": str(exc)}

// src/evaluation/pipeline.py
import json
from typing import List, Dict, Any, Optional
from tqdm import tqdm

from src.evaluation.evaluators import evaluate_translation_pair, check_context_mismatch
from src.clients.azure_client import chat

__all__ = ["run_evaluation_pipeline"]

AlignedPair = Dict[str, Any]
EvaluationFinding = Dict[str, Any]

def _agent2_validate_finding(
    eng_text: str,
    ger_text: str,
    error_type: str,
    explanation: str,
    model_name: Optional[str] = None,
):
    """
    Second-stage reviewer.  Confirms only truly fatal errors and rejects
    false positives.
    """
    prompt = f"""
## ROLE
**Senior Quality Reviewer** â€“ you are the final gatekeeper of ENâ†’DE
translation findings.

## TASK
Decide whether the finding delivered by Agent-1 must be *Confirmed* or
*Rejected*.

## INSTRUCTIONS
1. Eligible error_type values are **exactly**:
   â€¢ "Mistranslation"  
   â€¢ "Omission"

2. Confirm only when the evidence is unmistakable:
   â€¢ Mistranslation
       â€“ number mismatch (digit or word)  
       â€“ polarity flip / opposite meaning  
       â€“ actor/role inversion  
   â€¢ Omission
       â€“ English states an explicit count (â€œtwoâ€, â€œthreeâ€, â€œbothâ€ â€¦) **or**
         lists concrete items, and at least one item is *truly* missing in
         German (not conveyed by paraphrase).

3. Reject when:
   â€¢ Difference is stylistic or synonymous.  
   â€¢ Proper names / document titles are rendered with an accepted German
     equivalent (e.g. â€œNichtfinanzielle ErklÃ¤rungâ€).  
   â€¢ Alleged omission is actually present via paraphrase.  

## OUTPUT â€‘ JSON ONLY
json {{ "verdict" : "Confirm" | "Reject", "reasoning": "" }}

## MATERIAL TO REVIEW
English text:
\"\"\"{eng_text}\"\"\"

German text:
\"\"\"{ger_text}\"\"\"

Agent-1 proposed:
  error_type : {error_type}
  explanation: {explanation}

## YOUR RESPONSE
Return the JSON object only â€“ no extra text.
"""
    try:
        content = chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            model=model_name,
        )
        j0, j1 = content.find("{"), content.rfind("}") + 1
        verdict_json = json.loads(content[j0:j1])
        is_confirmed = verdict_json.get("verdict", "").lower() == "confirm"
        reasoning = verdict_json.get("reasoning", "")
        return is_confirmed, reasoning, content.strip()
    except (ValueError, json.JSONDecodeError) as exc:
        print(f"  - Agent-2 JSON parse error: {exc}")
        return False, f"System error: {exc}", "{}"
    except Exception as exc:
        print(f"  - Agent-2 unexpected error: {exc}")
        return False, "System error (non-parsing issue)", "{}"

def run_evaluation_pipeline(aligned_pairs: List[AlignedPair]) -> List[EvaluationFinding]:
    findings = []

    for pair in tqdm(aligned_pairs, desc="Evaluating Pairs"):
        eng_elem = pair.get('english')
        ger_elem = pair.get('german')

        if eng_elem and not ger_elem:
            findings.append({
                "type": f"Omission",
                "english_text": eng_elem['text'],
                "german_text": "---",
                "suggestion": "This content from the English document is missing in the German document.",
                "page": eng_elem['page']
            })
            continue

        if not eng_elem and ger_elem:
            findings.append({
                "type": f"Addition",
                "english_text": "---",
                "german_text": ger_elem['text'],
                "suggestion": "This content from the German document does not appear to have a source in the English document.",
                "page": ger_elem['page']
            })
            continue

        if eng_elem and ger_elem:
            eng_text = eng_elem['text']
            ger_text = ger_elem['text']

            # Agent 1
            finding = evaluate_translation_pair(eng_text, ger_text)
            error_type = finding.get("error_type", "None")

            if error_type not in ["None", "System Error"]:
                # Agent 2
                is_confirmed, reasoning, _ = _agent2_validate_finding(
                    eng_text, ger_text, error_type, finding.get("explanation")
                )

                if is_confirmed:
                    findings.append({
                        "type": error_type,
                        "english_text": eng_text,
                        "german_text": ger_text,
                        "suggestion": finding.get("suggestion"),
                        "page": eng_elem.get('page'),
                        "original_phrase": finding.get("original_phrase"),
                        "translated_phrase": finding.get("translated_phrase")
                    })
                else: # Agent 2 rejected, run Agent 3
                    context_result = check_context_mismatch(eng_text, ger_text)
                    context_match_verdict = context_result.get('context_match', 'Error')
                    if context_match_verdict.lower() == "no":
                        findings.append({
                            "type": "Context Mismatch",
                            "english_text": eng_text,
                            "german_text": ger_text,
                            "suggestion": context_result.get("explanation"),
                            "page": eng_elem.get('page')
                        })

    return findings

// src/processing/json_parser.py
import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED (FINAL FIX):
    This parser now iterates through ALL FOUR content lists in the JSON:
    - paragraphs (which includes text, list items, footnotes, etc.)
    - tables
    - figures (which includes charts, images, and their captions)
    - sections (which includes other document segments)
    
    It extracts all of them into one single, offset-sorted list
    by slicing the main 'content' string. A set is used to
    prevent duplicate entries.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        # The full markdown string is the single source of truth
        full_text_content = analyze_result.get('content', '')
        if not full_text_content:
             raise ValueError("Document Intelligence 'content' field is empty.")
             
        raw_paragraphs = analyze_result.get('paragraphs', [])
        raw_tables = analyze_result.get('tables', [])
        raw_figures = analyze_result.get('figures', [])
        raw_sections = analyze_result.get('sections', [])
        pages = analyze_result.get('pages', [])
        
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    # --- Step 1: Create a page lookup for all items ---
    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    # --- Step 2: Extract all content items by slicing the main content string ---
    all_content: List[ContentItem] = []
    
    # Set to store offsets we've already added to prevent duplication
    added_offsets = set()

    # Create a helper to process items from any list
    def add_item(item_list, default_role):
        for item in item_list:
            if not item.get('spans'):
                continue
            
            span = item['spans'][0]
            offset = span['offset']
            length = span['length']
            
            # Skip if we've already added this exact item
            if offset in added_offsets:
                continue

            # Get role, with fallbacks for different item types
            role = item.get('role', item.get('kind', default_role))

            if role in config.IGNORED_ROLES:
                continue

            text = full_text_content[offset : offset + length].strip()
            page_number = page_lookup.get(offset, 0)
            
            if text:
                all_content.append({
                    'text': text, 
                    'type': role, 
                    'page': page_number, 
                    'offset': offset
                })
                added_offsets.add(offset)

    # --- Process all content lists ---
    add_item(raw_paragraphs, 'paragraph')
    add_item(raw_tables, 'table')
    add_item(raw_figures, 'figure')
    add_item(raw_sections, 'section')

    # --- Step 3: Sort all extracted content by its character offset ---
    all_content.sort(key=lambda x: x['offset'])
    
    return all_content

// src/processing/section_parser.py
from typing import List, Dict, Any, Tuple

# Type Alias for clarity
ContentItem = Dict[str, Any]
HeadingPair = Tuple[ContentItem, ContentItem]
AlignedPair = Dict[str, Any] # The final output format for the evaluation pipeline

def _find_content_between(
    start_item: ContentItem, 
    end_item: ContentItem, 
    all_content: List[ContentItem]
) -> str:
    """
    Finds and concatenates all content items between two items,
    based on their character offsets.
    """
    start_offset = start_item['offset']
    end_offset = end_item['offset']
    
    # We want content *after* the start heading and *before* the end heading
    content_blocks = [
        item['text'] for item in all_content
        if start_offset < item['offset'] < end_offset
    ]
    
    return "\n\n".join(content_blocks)

def create_section_pairs(
    validated_heading_pairs: List[HeadingPair],
    full_english_content: List[ContentItem],
    full_german_content: List[ContentItem]
) -> List[AlignedPair]:
    """
    Main orchestration function for Phase 2:
    Uses the validated heading pairs as 'anchors' to extract
    all content *between* them.
    """
    if not validated_heading_pairs:
        return []

    print(f"Creating content sections from {len(validated_heading_pairs)} validated heading pairs...")
    
    final_aligned_pairs: List[AlignedPair] = []
    
    # Iterate through all consecutive pairs of headings
    for i in range(len(validated_heading_pairs) - 1):
        # Current pair (A)
        eng_heading_A, ger_heading_A = validated_heading_pairs[i]
        
        # Next pair (B)
        eng_heading_B, ger_heading_B = validated_heading_pairs[i+1]
        
        # Extract content between A and B for English
        eng_section_text = _find_content_between(
            eng_heading_A, 
            eng_heading_B, 
            full_english_content
        )
        
        # Extract content between A and B for German
        ger_section_text = _find_content_between(
            ger_heading_A, 
            ger_heading_B, 
            full_german_content
        )
        
        # --- MODIFICATION ---
        # The 'if eng_section_text or ger_section_text:' check has been removed.
        # We now append ALL sections, even if both are empty.
        
        final_aligned_pairs.append({
            "english": {
                "text": eng_section_text,
                "type": "section",
                "page": eng_heading_A['page'] # Use start heading page as reference
            },
            "german": {
                "text": ger_section_text,
                "type": "section",
                "page": ger_heading_A['page']
            },
            "similarity": 0.0, # Not applicable in this model
            "margin_score": 0.0 # Not applicable in this model
        })

    # Note: We currently don't process content *after* the last validated heading.
    
    # This number should now be (len(validated_heading_pairs) - 1)
    print(f"Created {len(final_aligned_pairs)} final section pairs for evaluation.")
    return final_aligned_pairs

// src/reporting/excel_writer.py
import io
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from itertools import zip_longest

# Type Aliases
AlignedPair = Dict[str, Any]
EvaluationFinding = Dict[str, Any]
ContentItem = Dict[str, Any]
MatchedHeading = Dict[str, Any]

# --- NEW: Function to save raw extracted headings ---
def save_raw_headings_report(
    eng_headings: List[ContentItem],
    ger_headings: List[ContentItem],
    filepath: Path
) -> None:
    """Saves the raw list of extracted headings to an Excel file."""
    eng_data = [{'text': h['text'], 'page': h['page'], 'offset': h['offset']} for h in eng_headings]
    ger_data = [{'text': h['text'], 'page': h['page'], 'offset': h['offset']} for h in ger_headings]

    combined_data = list(zip_longest(eng_data, ger_data, fillvalue={}))
    
    report_data = [
        {
            "English Heading": d[0].get('text', ''),
            "English Page": d[0].get('page', ''),
            "English Offset": d[0].get('offset', ''),
            "German Heading": d[1].get('text', ''),
            "German Page": d[1].get('page', ''),
            "German Offset": d[1].get('offset', ''),
        }
        for d in combined_data
    ]
    
    df = pd.DataFrame(report_data)
    try:
        df.to_excel(filepath, index=False, engine='openpyxl')
        print(f"-> Saved raw headings report to '{filepath}'")
    except Exception as e:
        print(f"Error: Could not write raw headings report to '{filepath}'. Reason: {e}")

# --- NEW: Function to save semantically matched headings ---
def save_matched_headings_report(
    matched_pairs: List[MatchedHeading],
    filepath: Path
) -> None:
    """Saves the semantically matched headings and their similarity scores."""
    report_data = []
    for pair in matched_pairs:
        eng = pair.get('english')
        ger = pair.get('german')
        report_data.append({
            "English Heading": eng['text'] if eng else "--- UNMATCHED ---",
            "English Page": eng['page'] if eng else "",
            "German Heading": ger['text'] if ger else "--- UNMATCHED ---",
            "German Page": ger['page'] if ger else "",
            "Cosine Similarity": f"{pair.get('similarity', 0.0):.4f}"
        })
    
    df = pd.DataFrame(report_data)
    df.sort_values(by="Cosine Similarity", ascending=False, inplace=True)
    try:
        df.to_excel(filepath, index=False, engine='openpyxl')
        print(f"-> Saved matched headings report to '{filepath}'")
    except Exception as e:
        print(f"Error: Could not write matched headings report to '{filepath}'. Reason: {e}")

# --- NEW: Function to save LLM-validated headings ---
def save_validated_headings_report(
    matched_pairs: List[MatchedHeading],
    filepath: Path
) -> None:
    """Saves the matched headings report, including the LLM validation status."""
    report_data = []
    for pair in matched_pairs:
        eng = pair.get('english')
        ger = pair.get('german')
        report_data.append({
            "English Heading": eng['text'] if eng else "--- UNMATCHED ---",
            "English Page": eng['page'] if eng else "",
            "German Heading": ger['text'] if ger else "--- UNMATCHED ---",
            "German Page": ger['page'] if ger else "",
            "Cosine Similarity": f"{pair.get('similarity', 0.0):.4f}",
            "LLM_Validated": pair.get('is_valid', False)
        })
    
    df = pd.DataFrame(report_data)
    df.sort_values(by=["LLM_Validated", "Cosine Similarity"], ascending=False, inplace=True)
    try:
        df.to_excel(filepath, index=False, engine='openpyxl')
        print(f"-> Saved validated headings report to '{filepath}'")
    except Exception as e:
        print(f"Error: Could not write validated headings report to '{filepath}'. Reason: {e}")

# --- NEW: Function to save the final section content pairs ---
def save_section_content_report(
    aligned_pairs: List[AlignedPair],
    filepath: Path
) -> None:
    """Saves the final aggregated section content before evaluation."""
    if not aligned_pairs:
        return
        
    report_data = []
    for pair in aligned_pairs:
        eng_item = pair.get('english')
        ger_item = pair.get('german')
        report_data.append({
            "English Section Content": eng_item['text'] if eng_item else "---",
            "German Section Content": ger_item['text'] if ger_item else "---",
            "Ref English Page": eng_item['page'] if eng_item else "N/A",
            "Ref German Page": ger_item['page'] if ger_item else "N/A"
        })
    
    df = pd.DataFrame(report_data)
    try:
        df.to_excel(filepath, index=False, engine='openpyxl')
        print(f"-> Saved final section content report to '{filepath}'")
    except Exception as e:
        print(f"Error: Could not write section content report to '{filepath}'. Reason: {e}")

# --- KEPT: Original function for the final evaluation report ---
def save_evaluation_report(evaluation_results: List[EvaluationFinding], filepath: Path) -> None:
    """Saves the AI evaluation findings to a separate Excel report."""
    if not evaluation_results:
        print("No evaluation findings to save.")
        return
    evaluation_results.sort(key=lambda x: x.get('page', 0))
    df = pd.DataFrame(evaluation_results)
    
    # Create the row data from the new AlignedPair structure
    report_data = []
    for finding in evaluation_results:
        report_data.append({
            "page": finding.get('page'),
            "type": finding.get('type'),
            "suggestion": finding.get('suggestion'),
            "english_text": finding.get('english_text'),
            "german_text": finding.get('german_text'),
            "original_phrase": finding.get('original_phrase'),
            "translated_phrase": finding.get('translated_phrase')
        })
        
    df = pd.DataFrame(report_data)
    desired_columns = [
        "page", "type", "suggestion", "english_text", "german_text",
        "original_phrase", "translated_phrase"
    ]
    final_columns = [col for col in desired_columns if col in df.columns]
    df = df[final_columns]
    
    try:
        df.to_excel(filepath, index=False, sheet_name='Evaluation_Findings')
    except Exception as e:
        print(f"Error: Could not write evaluation report to '{filepath}'. Reason: {e}")

# --- KEPT: Original function for the in-memory report (download) ---
def create_excel_report_in_memory(evaluation_results: List[EvaluationFinding]) -> bytes:
    """
    Creates the evaluation Excel report in memory and returns it as bytes.
    """
    if not evaluation_results:
        return b''

    evaluation_results.sort(key=lambda x: x.get('page', 0))
    
    # Create the row data from the new AlignedPair structure
    report_data = []
    for finding in evaluation_results:
        report_data.append({
            "page": finding.get('page'),
            "type": finding.get('type'),
            "suggestion": finding.get('suggestion'),
            "english_text": finding.get('english_text'),
            "german_text": finding.get('german_text'),
            "original_phrase": finding.get('original_phrase'),
            "translated_phrase": finding.get('translated_phrase')
        })
        
    df = pd.DataFrame(report_data)
    desired_columns = [
        "page", "type", "suggestion", "english_text", "german_text",
        "original_phrase", "translated_phrase"
    ]
    final_columns = [col for col in desired_columns if col in df.columns]
    df = df[final_columns]

    output_buffer = io.BytesIO()
    with pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Evaluation_Findings')

    return output_buffer.getvalue()

// src/reporting/markdown_writer.py
from pathlib import Path
from typing import List, Dict, Any

ContentItem = Dict[str, Any]

def save_to_markdown(content: List[ContentItem], filepath: Path) -> None:
    with open(filepath, 'w', encoding='utf-8') as f:
        for item in content:
            if item['type'] in {'title', 'sectionHeading', 'subheading'}:
                f.write(f"## {item['text']}\n\n")
            else:
                f.write(f"{item['text']}\n\n")

// app.py
from dotenv import load_dotenv
load_dotenv()

import streamlit as st
from datetime import datetime
import time
from pathlib import Path

# --- Import project modules ---
import config
from src.clients.doc_intelligence_client import analyze_pdf
from src.processing.json_parser import process_document_json
from src.alignment.heading_aligner import match_and_validate_headings
from src.processing.section_parser import create_section_pairs
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import (
    create_excel_report_in_memory,
    save_evaluation_report,
    save_section_content_report  # <-- Import the new function
)

# --- Page Configuration ---
st.set_page_config(page_title="Translation Evaluator", layout="wide")

# --- Using your provided display_results function ---
def display_results(results_list: list):
    """Renders the list of evaluation findings in the Streamlit UI."""
    if not results_list:
        return
    st.subheader(f"Found {len(results_list)} noteworthy items")
    results_list.sort(key=lambda x: x.get('page', 0))
    
    # --- MODIFICATION 1: Add enumerate to get a unique index 'i' ---
    for i, result in enumerate(results_list):
        error_type = result.get('type', 'Info')
        with st.container(border=True):
            st.markdown(f"**Page:** `{result.get('page', 'N/A')}` | **Type:** `{error_type}`")
            original_phrase, translated_phrase = result.get("original_phrase"), result.get("translated_phrase")
            
            if original_phrase or translated_phrase:
                st.markdown("##### ğŸ” Error Focus")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Original English Phrase:**")
                    st.error(f"'{original_phrase or 'N/A'}'")
                with col2:
                    st.markdown("**Translated German Phrase:**")
                    st.warning(f"'{translated_phrase or 'N/A'}'")
                st.divider()

            st.markdown("##### Full Section Context")
            col1, col2 = st.columns(2)
            with col1:
                # --- MODIFICATION 2: Add a unique key ---
                st.text_area(
                    "English Section", 
                    value=result['english_text'], 
                    height=200, 
                    disabled=True, 
                    key=f"eng_section_{i}"
                )
            with col2:
                # --- MODIFICATION 3: Add a unique key ---
                st.text_area(
                    "German Section", 
                    value=result['german_text'], 
                    height=200, 
                    disabled=True, 
                    key=f"ger_section_{i}"
                )
            
            st.markdown(f"**ğŸ’¡ Suggestion:** {result['suggestion']}")


# --- Main App ---
st.title("ğŸ“š Translation Evaluator")
st.markdown("This tool aligns and evaluates translated PDF documents using a **Headings-First** structural approach.")
st.divider()

if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = []
if 'error_message' not in st.session_state:
    st.session_state.error_message = None

# --- Sidebar for Inputs and Controls ---
with st.sidebar:
    st.header("1. Upload Documents")
    english_pdf = st.file_uploader("Upload English PDF (Source)", type="pdf", key="eng_pdf")
    german_pdf = st.file_uploader("Upload German PDF (Translation)", type="pdf", key="ger_pdf")

    st.header("2. Configure & Run")
    
    st.markdown("Specify the page number to **start** processing from. This is used to skip the Table of Contents.")
    eng_start_page = st.number_input("English Start Page", min_value=1, max_value=100, value=5)
    ger_start_page = st.number_input("German Start Page", min_value=1, max_value=100, value=5)

    if st.button("ğŸš€ Run Analysis", disabled=not (english_pdf and german_pdf), type="primary"):
        st.session_state.analysis_complete = False
        st.session_state.evaluation_results = []
        st.session_state.error_message = None

        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{Path(english_pdf.name).stem}_{timestamp}"

        try:
            # --- Start New Pipeline ---
            
            with st.spinner("Step 1/7: Analyzing PDFs with Azure Document Intelligence..."):
                eng_pdf_bytes = english_pdf.getvalue()
                ger_pdf_bytes = german_pdf.getvalue()
                eng_json_data = analyze_pdf(eng_pdf_bytes, english_pdf.name)
                ger_json_data = analyze_pdf(ger_pdf_bytes, german_pdf.name)

            with st.spinner("Step 2/7: Processing full document content..."):
                full_english_content = process_document_json(eng_json_data)
                full_german_content = process_document_json(ger_json_data)
                st.toast(f"Extracted {len(full_english_content)} EN segments and {len(full_german_content)} DE segments.")

            with st.spinner("Step 3/7: Matching and validating section headings (Phase 1)..."):
                validated_heading_pairs = match_and_validate_headings(
                    full_english_content,
                    full_german_content,
                    eng_start_page,
                    ger_start_page,
                    output_dir,
                    base_filename
                )
                st.toast(f"Found {len(validated_heading_pairs)} validated heading pairs.")
                with st.expander("âœ… Validated Heading Matches", expanded=True):
                    for eng_h, ger_h in validated_heading_pairs[:20]: # Show first 20
                        st.write(f"'{eng_h['text']}' â†’ '{ger_h['text']}'")
                    if len(validated_heading_pairs) > 20:
                        st.write(f"...and {len(validated_heading_pairs) - 20} more.")

            with st.spinner("Step 4/7: Creating content pairs from sections (Phase 2)..."):
                final_aligned_pairs = create_section_pairs(
                    validated_heading_pairs,
                    full_english_content,
                    full_german_content
                )
                st.toast(f"Created {len(final_aligned_pairs)} section pairs for evaluation.")

            with st.spinner("Step 5/7: Saving section content debug report..."):
                if final_aligned_pairs:
                    content_report_path = output_dir / f"content_pairs_{base_filename}.xlsx"
                    save_section_content_report(final_aligned_pairs, content_report_path)
                    st.toast("Saved Section Content Report.")

            with st.spinner("Step 6/7: Evaluating aligned sections for errors..."):
                if not final_aligned_pairs:
                    st.warning("No content sections were created. Evaluation will be skipped.")
                    st.session_state.evaluation_results = []
                else:
                    st.session_state.evaluation_results = list(run_evaluation_pipeline(final_aligned_pairs))

            with st.spinner("Step 7/7: Saving final evaluation report..."):
                if st.session_state.evaluation_results:
                    eval_report_path = output_dir / f"evaluation_{base_filename}.xlsx"
                    save_evaluation_report(st.session_state.evaluation_results, eval_report_path)
                    st.toast("Saved Final Evaluation Report.")

            # --- End New Pipeline ---

            st.session_state.analysis_complete = True
            st.success("Analysis pipeline finished successfully!")
            time.sleep(2)
            st.rerun()

        except Exception as e:
            st.session_state.error_message = f"An error occurred: {e}"
            st.exception(e) 
            st.rerun()
    
    # --- Download Button ---
    st.header("3. Export Results")
    if st.session_state.analysis_complete and st.session_state.evaluation_results:
        excel_data = create_excel_report_in_memory(st.session_state.evaluation_results)
        st.download_button(
            label="ğŸ“¥ Download Evaluation Report",
            data=excel_data,
            file_name=f"Translation_Evaluation_{datetime.now().strftime('%Y-%m-%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        st.markdown("_Report available after analysis._")

# --- Main Display Area ---
st.header("Evaluation Results")

if st.session_state.error_message:
    st.error(st.session_state.error_message, icon="ğŸš¨")
elif st.session_state.analysis_complete:
    if not st.session_state.evaluation_results:
        st.success("âœ… Analysis complete. No significant errors were found.")
    else:
        display_results(st.session_state.evaluation_results)
else:
    st.info("Upload your PDFs, set the start pages, and click 'Run Analysis' to begin.")

// main.py
import argparse
import time
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()

import config
from src.processing.json_parser import process_document_json
from src.alignment.heading_aligner import match_and_validate_headings
from src.processing.section_parser import create_section_pairs
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import (
    save_evaluation_report, 
    save_section_content_report  # <-- Import the new function
)
from src.reporting.markdown_writer import save_to_markdown

def main():
    parser = argparse.ArgumentParser(
        description="Aligns and evaluates content from two Document Intelligence JSONs using a 'Headings-First' approach."
    )
    parser.add_argument("english_json", type=str, help="Path to the English JSON file.")
    parser.add_argument("german_json", type=str, help="Path to the German JSON file.")
    parser.add_argument(
        "--eng_start_page", type=int, default=1,
        help="Page number to start processing from in the English document (to skip ToC)."
    )
    parser.add_argument(
        "--ger_start_page", type=int, default=1,
        help="Page number to start processing from in the German document (to skip ToC)."
    )
    parser.add_argument(
        "--evaluate", action="store_true",
        help="Run the AI evaluation pipeline on the aligned sections."
    )
    args = parser.parse_args()

    # --- 1. Setup Paths ---
    eng_path = Path(args.english_json)
    ger_path = Path(args.german_json)

    output_dir = Path(config.OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    base_filename = f"{eng_path.stem}_{timestamp}"

    print("--- 'Headings-First' Alignment Pipeline Started ---")
    print(f"English Source: {eng_path}")
    print(f"German Source:  {ger_path}")
    print(f"English Start Page: {args.eng_start_page}")
    print(f"German Start Page:  {args.ger_start_page}\n")

    try:
        # --- Start New Pipeline ---

        print("Step 1/6: Processing JSON files...")
        full_english_content = process_document_json(eng_path)
        full_german_content = process_document_json(ger_path)
        print(f"-> Extracted {len(full_english_content)} EN segments and {len(full_german_content)} DE segments.\n")

        print("Step 2/6: Creating verification Markdown files...")
        save_to_markdown(full_english_content, output_dir / f"{eng_path.stem}_processed.md")
        save_to_markdown(full_german_content, output_dir / f"{ger_path.stem}_processed.md")
        print(f"-> Markdown files saved in '{output_dir.resolve()}'\n")

        print("Step 3/6: Matching and validating section headings (Phase 1)...")
        # This function saves its own 3 debug reports
        validated_heading_pairs = match_and_validate_headings(
            full_english_content,
            full_german_content,
            args.eng_start_page,
            args.ger_start_page,
            output_dir,
            base_filename
        )
        print(f"-> Found {len(validated_heading_pairs)} validated heading pairs.\n")

        print("Step 4/6: Creating content pairs from sections (Phase 2)...")
        final_aligned_pairs = create_section_pairs(
            validated_heading_pairs,
            full_english_content,
            full_german_content
        )
        print(f"-> Created {len(final_aligned_pairs)} section pairs for evaluation.\n")

        print("Step 5/6: Saving section content debug report...")
        if final_aligned_pairs:
            content_report_path = output_dir / f"content_pairs_{base_filename}.xlsx"
            save_section_content_report(final_aligned_pairs, content_report_path)

        if args.evaluate:
            print("Step 6/6: Running AI evaluation pipeline...")
            if not final_aligned_pairs:
                print("-> No content sections were created. Evaluation will be skipped.")
            else:
                evaluation_results = list(run_evaluation_pipeline(final_aligned_pairs))
                
                if not evaluation_results:
                    print("-> Evaluation complete. No significant errors were found.")
                else:
                    print(f"-> Evaluation complete. Found {len(evaluation_results)} potential errors.")
                    output_eval_path = output_dir / f"evaluation_report_{base_filename}.xlsx"
                    save_evaluation_report(evaluation_results, output_eval_path)
                    print(f"-> Evaluation report saved to: {output_eval_path.resolve()}")
        else:
            print("Step 6/6: Skipping evaluation as --evaluate flag was not set.")

    except FileNotFoundError as e:
        print(f"Error: Input file not found. {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred during the pipeline: {e}")
        return

    print("\n--- Pipeline Finished Successfully ---")

if __name__ == "__main__":
    main()

// requirements.txt
streamlit
openai
azure-core
python-dotenv
azure-ai-documentintelligence

# Libraries for data handling and calculations
numpy
scikit-learn  # Used for cosine_similarity
tqdm
scipy         # Used for linear_sum_assignment (Hungarian algorithm)

# Libraries for dataframes and writing Excel files
pandas
openpyxl

// test.py
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import sys
from datetime import datetime

# --- CONFIGURATION ---
# These are the files you have provided.
# Make sure they are in the correct relative paths.
# The JSON file from Document Intelligence
JSON_PATH = "output/doc_intelligence_en-allianz-se-annual-report-2024_20251103_103805.json"
# The EXCEL file of validated headings
HEADINGS_CSV_PATH = "output/headings_2_validated_doc_intelligence_en-allianz-se-annual-report-2024_20251103_103805_20251104_095415.xlsx"
# The page numbers we want to investigate (CHANGED: Now 3-115)
PAGES_TO_DEBUG = set(range(3, 116))  # Pages 3 to 115 inclusive
# Output file for debug results (Timestamped)
OUTPUT_FILE = f"output/debug_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# --- 1. RE-IMPLEMENTATION OF JSON PARSER ---
# This is the last version of the parser logic, which is likely correct.

IGNORED_ROLES = {'pageHeader', 'pageFooter', 'pageNumber'}
ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> (List[ContentItem], str):
    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        full_text_content = analyze_result.get('content', '')
        if not full_text_content:
             raise ValueError("Document Intelligence 'content' field is empty.")

        raw_paragraphs = analyze_result.get('paragraphs', [])
        raw_tables = analyze_result.get('tables', [])
        raw_figures = analyze_result.get('figures', [])
        raw_sections = analyze_result.get('sections', [])
        pages = analyze_result.get('pages', [])
    except KeyError as e:
        raise ValueError(f"Data is missing expected key: {e}") from e

    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    all_content: List[ContentItem] = []
    added_offsets = set()

    def add_item(item_list, default_role):
        for item in item_list:
            if not item.get('spans'):
                continue

            span = item['spans'][0]
            offset = span['offset']
            length = span['length']

            if offset in added_offsets:
                continue

            role = item.get('role', item.get('kind', default_role))

            if role in IGNORED_ROLES:
                continue

            text = full_text_content[offset : offset + length].strip()
            page_number = page_lookup.get(offset, 0)

            if text:
                all_content.append({
                    'text': text, 
                    'type': role, 
                    'page': page_number, 
                    'offset': offset
                })
                added_offsets.add(offset)

    add_item(raw_paragraphs, 'paragraph')
    add_item(raw_tables, 'table')
    add_item(raw_figures, 'figure')
    add_item(raw_sections, 'section')

    all_content.sort(key=lambda x: x['offset'])

    print(f"[DEBUG] json_parser.py: Extracted {len(all_content)} total content items.")
    return all_content, full_text_content

# --- 2. RE-IMPLEMENTATION OF SECTION PARSER (WITH DEBUGGING) ---
def _find_content_between(
    start_item: ContentItem, 
    end_item: ContentItem, 
    all_content: List[ContentItem],
    full_text_content: str):
    """
    Finds and concatenates all content items between two items,
    based on their character offsets.
    """
    start_offset = start_item['offset']
    end_offset = end_item['offset']

    # This is the core logic. We find all items from the *full* list
    # that fall between our two headings.
    content_items_found = [
        item for item in all_content
        if start_offset < item['offset'] < end_offset
    ]

    # --- DETAILED DEBUGGING FOR BLANK SECTIONS ---
    if start_item['page'] in PAGES_TO_DEBUG:
        print("\n" + "="*80)
        print(f"DEBUGGING PAGE: {start_item['page']}")
        print(f"  Start Heading: '{start_item['text'][:50]}...' (Offset: {start_offset}, Page: {start_item['page']})")
        print(f"  End Heading:   '{end_item['text'][:50]}...' (Offset: {end_offset}, Page: {end_item['page']})")

        # This is the critical test.
        # We will check if any *un-validated* headings exist in this gap.
        all_headings_in_gap = [
            item for item in content_items_found 
            if item['type'] in {'sectionHeading', 'title'}
        ]

        if not content_items_found:
            print("  [FLAW IDENTIFIED?] No content items of ANY type were found between these two offsets.")

            # Let's check the raw markdown string
            # Check from the character *after* the start heading's text to the end heading's start
            raw_text_between = full_text_content[start_offset + len(start_item['text']):end_offset].strip()
            if raw_text_between:
                 print(f"  [RAW TEXT FOUND] '{raw_text_between[:100]}...'")
                 print("  [CONCLUSION] The json_parser IS failing. It's not extracting items that exist in the raw text.")
            else:
                 print("  [RAW TEXT EMPTY] The raw markdown string is also empty between these headings.")
                 print("  [CONCLUSION] The headings are back-to-back. The section is truly empty.")

        elif all_headings_in_gap:
            print(f"  [FLAW IDENTIFIED?] Found {len(all_headings_in_gap)} other headings in this gap:")
            for h in all_headings_in_gap:
                print(f"    - (Page {h['page']}) {h['text'][:70]}...")
            print("  [CONCLUSION] The problem is in heading_aligner.py. It failed to validate these headings,")
            print("  so section_parser.py is skipping over them and creating a giant, incorrect section.")

        else:
            print(f"  [INFO] Found {len(content_items_found)} valid content items (paragraphs, tables, etc.).")
            # Only print the conclusion if the section is truly blank, otherwise the output gets too verbose.
            if not content_items_found: 
                print("  [CONCLUSION] This section should NOT be blank. The flaw is somewhere else.")

        print("="*80 + "\n")

    content_blocks = [item['text'] for item in content_items_found]
    return "\n\n".join(content_blocks)

# --- 3. MAIN DEBUG RUNNER ---
def run_debug():
    
    # --- Inner Class: Tee (For simultaneous console/file output) ---
    class Tee:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()

    # Create output directory if it doesn't exist
    Path("output").mkdir(exist_ok=True)

    # Open output file
    original_stdout = sys.stdout
    
    try:
        # Redirect all print output to both console and file
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            sys.stdout = Tee(original_stdout, f)

            print("--- STARTING DEBUG RUN ---")
            print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"Debugging pages: {min(PAGES_TO_DEBUG)} to {max(PAGES_TO_DEBUG)}")
            print(f"Output file: {OUTPUT_FILE}\n")

            # --- Load JSON ---
            try:
                with open(JSON_PATH, 'r', encoding='utf-8') as json_f:
                    doc_data = json.load(json_f)
                print(f"Successfully loaded JSON: {JSON_PATH}")
            except Exception as e:
                print(f"FATAL: Could not load JSON file. {e}")
                return

            # --- Load Validated Headings EXCEL FILE ---
            try:
                # Use openpyxl engine
                headings_df = pd.read_excel(HEADINGS_CSV_PATH, engine='openpyxl')
                validated_df = headings_df[headings_df['LLM_Validated'] == True].copy() # use .copy() to avoid SettingWithCopyWarning
                print(f"Successfully loaded Excel file: {HEADINGS_CSV_PATH}")
                print(f"Found {len(validated_df)} validated headings to process.\n")
            except Exception as e:
                print(f"FATAL: Could not load Headings Excel file. {e}")
                return

            # --- Run JSON Parser ---
            all_content_items, full_text_content = process_document_json(doc_data)

            # We must rebuild the `validated_heading_pairs` list
            # This requires matching the CSV rows back to the full content items

            # Create a lookup of all content items by (page, text)
            content_lookup = {}
            for item in all_content_items:
                # Use a simplified/truncated text for the lookup key to be robust to minor whitespace changes
                key = (item['page'], item['text'].strip())
                content_lookup[key] = item

            # Create the list of (eng_item) tuples
            validated_eng_headings = []
            not_found_count = 0
            for _, row in validated_df.iterrows():
                # Clean the text from the excel file as well
                key = (row['English Page'], row['English Heading'].strip())
                if key in content_lookup:
                    validated_eng_headings.append(content_lookup[key])
                else:
                    not_found_count += 1
            
            # --- Robustness check for case where the text doesn't exactly match ---
            if not_found_count > 0:
                print(f"Warning: {not_found_count} validated headings could not be matched to content items (exact text/page).")
                print("Proceeding with only matched headings.\n")

            # Sort by offset, just like the real script
            validated_eng_headings.sort(key=lambda x: x['offset'])

            print(f"--- Running Section Parser Logic for {len(validated_eng_headings)} English Headings ---\n")

            # --- Run Section Parser ---
            total_blank_sections = 0
            total_sections_checked = 0
            blank_sections_in_debug_range = 0

            for i in range(len(validated_eng_headings) - 1):
                start_heading = validated_eng_headings[i]
                end_heading = validated_eng_headings[i+1]

                section_text = _find_content_between(
                    start_heading, 
                    end_heading, 
                    all_content_items,
                    full_text_content
                )

                total_sections_checked += 1

                if not section_text:
                    total_blank_sections += 1
                    if start_heading['page'] in PAGES_TO_DEBUG:
                        blank_sections_in_debug_range += 1
            
            # --- FINAL SUMMARY ---
            print(f"\n{'='*80}")
            print(f"--- DEBUG RUN COMPLETE ---")
            print(f"{'='*80}")
            print(f"Total Sections Checked: {total_sections_checked}")
            print(f"Total Blank Sections Found (entire document): {total_blank_sections}")
            print(f"Blank Sections in Debug Range (pages {min(PAGES_TO_DEBUG)}-{max(PAGES_TO_DEBUG)}): {blank_sections_in_debug_range}")
            # Protect against division by zero in case only one heading was found (unlikely, but safe)
            if total_sections_checked > 0:
                 print(f"\nPercentage of blank sections: {(total_blank_sections/total_sections_checked * 100):.2f}%")
            print(f"\nDetailed debug output saved to: {OUTPUT_FILE}")
            print("Review the 'DEBUGGING PAGE' sections above to identify the flaw.")
            print(f"{'='*80}\n")
    finally:
        # Restore original stdout
        sys.stdout = original_stdout
        print(f"\nâœ“ Debug complete! Results saved to: {OUTPUT_FILE}")

if __name__ == "__main__": 
    # Make sure to install the necessary libraries if you haven't already:
    # pip install pandas openpyxl
    run_debug()

