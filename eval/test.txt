import json
from typing import List, Dict, Any, Tuple
import numpy as np
from scipy.optimize import linear_sum_assignment
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Import clients and report writers
from src.clients.azure_client import get_embeddings, chat
from src.reporting.excel_writer import (
    save_raw_headings_report, 
    save_matched_headings_report,
    save_validated_headings_report
)

# Type Alias for clarity
ContentItem = Dict[str, Any]
HeadingPair = Tuple[ContentItem, ContentItem]

def _validate_heading_pair(eng_heading: str, ger_heading: str) -> bool:
    """
    Uses a 'liberal' LLM prompt to check if two headings are plausible translations.
    """
    prompt = f"""
## ROLE
You are a fast, liberal document analyst. Your goal is to check if two section headings *could* plausibly refer to the same section, even if they aren't exact translations.

## TASK
Compare the English and German headings. Respond with a single JSON object containing one key, "match", with a value of "Yes" or "No".

## RULES
- Respond "Yes" if they are clear translations (e.g., "Board Report" -> "Bericht des Vorstands").
- Respond "Yes" if they refer to the same concept (e.g., "Corporate Governance" -> "ErklÃ¤rung zur UnternehmensfÃ¼hrung").
- Respond "Yes" if one is a summary of the other (e.g., "Notes to the Financial Statements" -> "Anhang").
- Respond "No" *only* if they are clearly about different topics (e.g., "Risk Report" -> "Human Resources").

## HEADINGS
<English>
{eng_heading}
</English>

<German>
{ger_heading}
</German>

## RESPONSE (JSON ONLY)
{{ "match": "Yes" | "No" }}
"""
    try:
        content = chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        ).strip()
        
        j0, j1 = content.find("{"), content.rfind("}") + 1
        result = json.loads(content[j0:j1])
        return result.get("match", "No").lower() == "yes"
        
    except Exception as e:
        print(f"Warning: Heading validation LLM call failed. Defaulting to 'No'. Error: {e}")
        return False

def match_and_validate_headings(
    english_content: List[ContentItem],
    german_content: List[ContentItem],
    eng_start_page: int,
    ger_start_page: int,
    output_dir: Any, # Expects a Path object
    base_filename: str
) -> List[HeadingPair]:
    """
    Main orchestration function for Phase 1:
    1. Filters for headings after the start page.
    2. Gets embeddings and matches them using the Hungarian algorithm.
    3. Validates all matches using an LLM.
    4. Saves debug reports at each step.
    """
    
    # 1. Filter for headings *after* the specified start page
    print(f"Filtering headings. EN start page: {eng_start_page}, DE start page: {ger_start_page}")
    eng_headings = [
        item for item in english_content 
        if item['type'] in {'title', 'sectionHeading'} and item['page'] >= eng_start_page
    ]
    ger_headings = [
        item for item in german_content 
        if item['type'] in {'title', 'sectionHeading'} and item['page'] >= ger_start_page
    ]
    
    if not eng_headings or not ger_headings:
        print("Error: No headings found after the start page in one or both documents.")
        return []

    # Save debug report 1: Raw Headings
    save_raw_headings_report(eng_headings, ger_headings, output_dir / f"headings_0_raw_{base_filename}.xlsx")

    # 2. Get Embeddings
    print("Getting embeddings for headings...")
    eng_heading_texts = [h['text'] for h in eng_headings]
    ger_heading_texts = [h['text'] for h in ger_headings]
    
    eng_embeddings = np.array(get_embeddings(eng_heading_texts))
    ger_embeddings = np.array(get_embeddings(ger_heading_texts))
    
    # 3. Calculate Cost Matrix and run Hungarian Algorithm
    print("Calculating similarity and running Hungarian algorithm...")
    similarity_matrix = cosine_similarity(eng_embeddings, ger_embeddings)
    cost_matrix = -similarity_matrix  # We want to maximize similarity, so minimize negative similarity
    
    row_indices, col_indices = linear_sum_assignment(cost_matrix)
    
    # 4. Create initial matched list and save debug report 2
    matched_pairs = []
    unmatched_eng = list(range(len(eng_headings)))
    unmatched_ger = list(range(len(ger_headings)))

    for eng_idx, ger_idx in zip(row_indices, col_indices):
        similarity = similarity_matrix[eng_idx, ger_idx]
        matched_pairs.append({
            "english": eng_headings[eng_idx],
            "german": ger_headings[ger_idx],
            "similarity": similarity
        })
        if eng_idx in unmatched_eng:
            unmatched_eng.remove(eng_idx)
        if ger_idx in unmatched_ger:
            unmatched_ger.remove(ger_idx)
            
    # Add unmatched items for the report
    for eng_idx in unmatched_eng:
        matched_pairs.append({"english": eng_headings[eng_idx], "german": None, "similarity": 0.0})
    for ger_idx in unmatched_ger:
        matched_pairs.append({"english": None, "german": ger_headings[ger_idx], "similarity": 0.0})

    save_matched_headings_report(matched_pairs, output_dir / f"headings_1_matched_{base_filename}.xlsx")

    # 5. Validate pairs using LLM
    print(f"Validating {len(row_indices)} matched pairs with LLM...")
    validated_pairs: List[HeadingPair] = []
    
    for pair in tqdm(matched_pairs, desc="Validating Headings"):
        eng_item = pair.get('english')
        ger_item = pair.get('german')
        
        # Only validate actual pairs
        if eng_item and ger_item:
            is_valid = _validate_heading_pair(eng_item['text'], ger_item['text'])
            pair['is_valid'] = is_valid
            if is_valid:
                # Add the tuple of full ContentItems
                validated_pairs.append((eng_item, ger_item))
        else:
            pair['is_valid'] = False

    # Save debug report 3: Validated Headings
    save_validated_headings_report(matched_pairs, output_dir / f"headings_2_validated_{base_filename}.xlsx")

    print(f"Found {len(validated_pairs)} validated heading pairs.")
    
    # Sort by English document order before returning
    validated_pairs.sort(key=lambda pair: pair[0]['offset'])
    
    return validated_pairs














from typing import List, Dict, Any, Tuple

# Type Alias for clarity
ContentItem = Dict[str, Any]
HeadingPair = Tuple[ContentItem, ContentItem]
AlignedPair = Dict[str, Any] # The final output format for the evaluation pipeline

def _find_content_between(
    start_item: ContentItem, 
    end_item: ContentItem, 
    all_content: List[ContentItem]
) -> str:
    """
    Finds and concatenates all content items between two items,
    based on their character offsets.
    """
    start_offset = start_item['offset']
    end_offset = end_item['offset']
    
    # We want content *after* the start heading and *before* the end heading
    content_blocks = [
        item['text'] for item in all_content
        if start_offset < item['offset'] < end_offset
    ]
    
    return "\n\n".join(content_blocks)

def create_section_pairs(
    validated_heading_pairs: List[HeadingPair],
    full_english_content: List[ContentItem],
    full_german_content: List[ContentItem]
) -> List[AlignedPair]:
    """
    Main orchestration function for Phase 2:
    Uses the validated heading pairs as 'anchors' to extract
    all content *between* them.
    """
    if not validated_heading_pairs:
        return []

    print(f"Creating content sections from {len(validated_heading_pairs)} validated heading pairs...")
    
    final_aligned_pairs: List[AlignedPair] = []
    
    # Iterate through all consecutive pairs of headings
    for i in range(len(validated_heading_pairs) - 1):
        # Current pair (A)
        eng_heading_A, ger_heading_A = validated_heading_pairs[i]
        
        # Next pair (B)
        eng_heading_B, ger_heading_B = validated_heading_pairs[i+1]
        
        # Extract content between A and B for English
        eng_section_text = _find_content_between(
            eng_heading_A, 
            eng_heading_B, 
            full_english_content
        )
        
        # Extract content between A and B for German
        ger_section_text = _find_content_between(
            ger_heading_A, 
            ger_heading_B, 
            full_german_content
        )
        
        # We create a pair even if one side is empty,
        # as this represents a structural Omission/Addition.
        if eng_section_text or ger_section_text:
            final_aligned_pairs.append({
                "english": {
                    "text": eng_section_text,
                    "type": "section",
                    "page": eng_heading_A['page'] # Use start heading page as reference
                },
                "german": {
                    "text": ger_section_text,
                    "type": "section",
                    "page": ger_heading_A['page']
                },
                "similarity": 0.0, # Not applicable in this model
                "margin_score": 0.0 # Not applicable in this model
            })

    # Note: We currently don't process content *after* the last validated heading.
    # This is assumed to be appendices, etc., but could be added if needed.
    
    print(f"Created {len(final_aligned_pairs)} final section pairs for evaluation.")
    return final_aligned_pairs
















from dotenv import load_dotenv
load_dotenv()

import streamlit as st
from datetime import datetime
import time
from pathlib import Path

# --- Import project modules ---
import config
from src.clients.doc_intelligence_client import analyze_pdf
from src.processing.json_parser import process_document_json
from src.alignment.heading_aligner import match_and_validate_headings
from src.processing.section_parser import create_section_pairs
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import (
    create_excel_report_in_memory,
    save_evaluation_report
)
# Note: Other alignment/debug reports are saved by the heading_aligner itself

# --- Page Configuration & UI Functions (unchanged from original) ---
st.set_page_config(page_title="Translation Evaluator", layout="wide")

def display_results(results_list: list):
    """Renders the list of evaluation findings in the Streamlit UI."""
    if not results_list:
        return
    st.subheader(f"Found {len(results_list)} noteworthy items")
    results_list.sort(key=lambda x: x.get('page', 0))
    for result in results_list:
        error_type = result.get('type', 'Info')
        with st.container(border=True):
            st.markdown(f"**Page:** `{result.get('page', 'N/A')}` | **Type:** `{error_type}`")
            original_phrase, translated_phrase = result.get("original_phrase"), result.get("translated_phrase")
            
            # Show error focus only if phrases are present
            if original_phrase or translated_phrase:
                st.markdown("##### ðŸ” Error Focus")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Original English Phrase:**")
                    st.error(f"'{original_phrase or 'N/A'}'")
                with col2:
                    st.markdown("**Translated German Phrase:**")
                    st.warning(f"'{translated_phrase or 'N/A'}'")
                st.divider()

            st.markdown("##### Full Section Context")
            col1, col2 = st.columns(2)
            with col1:
                # Use st.text() or st.code_block() for large text blocks to respect formatting
                st.text_area("English Section", value=result['english_text'], height=200, disabled=True)
            with col2:
                st.text_area("German Section", value=result['german_text'], height=200, disabled=True)
            
            st.markdown(f"**ðŸ’¡ Suggestion:** {result['suggestion']}")


# --- Main App ---
st.title("ðŸ“š Translation Evaluator")
st.markdown("This tool aligns and evaluates translated PDF documents using a **Headings-First** structural approach.")
st.divider()

if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = []
if 'error_message' not in st.session_state:
    st.session_state.error_message = None

# --- Sidebar for Inputs and Controls ---
with st.sidebar:
    st.header("1. Upload Documents")
    english_pdf = st.file_uploader("Upload English PDF (Source)", type="pdf", key="eng_pdf")
    german_pdf = st.file_uploader("Upload German PDF (Translation)", type="pdf", key="ger_pdf")

    st.header("2. Configure & Run")
    
    st.markdown("Specify the page number to **start** processing from. This is used to skip the Table of Contents.")
    eng_start_page = st.number_input("English Start Page", min_value=1, max_value=100, value=5)
    ger_start_page = st.number_input("German Start Page", min_value=1, max_value=100, value=5)

    if st.button("ðŸš€ Run Analysis", disabled=not (english_pdf and german_pdf), type="primary"):
        st.session_state.analysis_complete = False
        st.session_state.evaluation_results = []
        st.session_state.error_message = None

        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{Path(english_pdf.name).stem}_{timestamp}"

        try:
            # --- Start New Pipeline ---
            
            with st.spinner("Step 1/6: Analyzing PDFs with Azure Document Intelligence..."):
                eng_pdf_bytes = english_pdf.getvalue()
                ger_pdf_bytes = german_pdf.getvalue()
                eng_json_data = analyze_pdf(eng_pdf_bytes, english_pdf.name)
                ger_json_data = analyze_pdf(ger_pdf_bytes, german_pdf.name)

            with st.spinner("Step 2/6: Processing full document content..."):
                # This parser is now simpler, just extracts all items
                full_english_content = process_document_json(eng_json_data)
                full_german_content = process_document_json(ger_json_data)
                st.toast(f"Extracted {len(full_english_content)} EN segments and {len(full_german_content)} DE segments.")

            with st.spinner("Step 3/6: Matching and validating section headings (Phase 1)..."):
                # This function now does the alignment AND saves its own debug reports
                validated_heading_pairs = match_and_validate_headings(
                    full_english_content,
                    full_german_content,
                    eng_start_page,
                    ger_start_page,
                    output_dir,
                    base_filename
                )
                st.toast(f"Found {len(validated_heading_pairs)} validated heading pairs.")
                with st.expander("âœ… Validated Heading Matches", expanded=True):
                    for eng_h, ger_h in validated_heading_pairs[:20]: # Show first 20
                        st.write(f"'{eng_h['text']}' â†’ '{ger_h['text']}'")
                    if len(validated_heading_pairs) > 20:
                        st.write(f"...and {len(validated_heading_pairs) - 20} more.")

            with st.spinner("Step 4/6: Creating content pairs from sections (Phase 2)..."):
                final_aligned_pairs = create_section_pairs(
                    validated_heading_pairs,
                    full_english_content,
                    full_german_content
                )
                st.toast(f"Created {len(final_aligned_pairs)} section pairs for evaluation.")

            with st.spinner("Step 5/6: Evaluating aligned sections for errors..."):
                if not final_aligned_pairs:
                    st.warning("No content sections were created. Evaluation will be skipped.")
                    st.session_state.evaluation_results = []
                else:
                    st.session_state.evaluation_results = list(run_evaluation_pipeline(final_aligned_pairs))

            with st.spinner("Step 6/6: Saving final evaluation report..."):
                if st.session_state.evaluation_results:
                    eval_report_path = output_dir / f"evaluation_{base_filename}.xlsx"
                    save_evaluation_report(st.session_state.evaluation_results, eval_report_path)
                    st.toast("Saved Final Evaluation Report.")

            # --- End New Pipeline ---

            st.session_state.analysis_complete = True
            st.success("Analysis pipeline finished successfully!")
            time.sleep(2)
            st.rerun()

        except Exception as e:
            st.session_state.error_message = f"An error occurred: {e}"
            st.exception(e) 
            st.rerun()
    
    # --- Download Button ---
    st.header("3. Export Results")
    if st.session_state.analysis_complete and st.session_state.evaluation_results:
        excel_data = create_excel_report_in_memory(st.session_state.evaluation_results)
        st.download_button(
            label="ðŸ“¥ Download Evaluation Report",
            data=excel_data,
            file_name=f"Translation_Evaluation_{datetime.now().strftime('%Y-%m-%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        st.markdown("_Report available after analysis._")

# --- Main Display Area ---
st.header("Evaluation Results")

if st.session_state.error_message:
    st.error(st.session_state.error_message, icon="ðŸš¨")
elif st.session_state.analysis_complete:
    if not st.session_state.evaluation_results:
        st.success("âœ… Analysis complete. No significant errors were found.")
    else:
        display_results(st.session_state.evaluation_results)
else:
    st.info("Upload your PDFs, set the start pages, and click 'Run Analysis' to begin.")








