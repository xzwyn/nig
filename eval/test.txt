import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED (FINAL FIX):
    This parser now iterates through ALL FOUR content lists in the JSON:
    - paragraphs (which includes text, list items, footnotes, etc.)
    - tables
    - figures (which includes charts, images, and their captions)
    - sections (which includes other document segments)
    
    It extracts all of them into one single, offset-sorted list
    by slicing the main 'content' string. A set is used to
    prevent duplicate entries.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        # The full markdown string is the single source of truth
        full_text_content = analyze_result.get('content', '')
        if not full_text_content:
             raise ValueError("Document Intelligence 'content' field is empty.")
             
        raw_paragraphs = analyze_result.get('paragraphs', [])
        raw_tables = analyze_result.get('tables', [])
        raw_figures = analyze_result.get('figures', [])
        raw_sections = analyze_result.get('sections', [])
        pages = analyze_result.get('pages', [])
        
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    # --- Step 1: Create a page lookup for all items ---
    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    # --- Step 2: Extract all content items by slicing the main content string ---
    all_content: List[ContentItem] = []
    
    # Set to store offsets we've already added to prevent duplication
    added_offsets = set()

    # Create a helper to process items from any list
    def add_item(item_list, default_role):
        for item in item_list:
            if not item.get('spans'):
                continue
            
            span = item['spans'][0]
            offset = span['offset']
            length = span['length']
            
            # Skip if we've already added this exact item
            if offset in added_offsets:
                continue

            # Get role, with fallbacks for different item types
            role = item.get('role', item.get('kind', default_role))

            if role in config.IGNORED_ROLES:
                continue

            text = full_text_content[offset : offset + length].strip()
            page_number = page_lookup.get(offset, 0)
            
            if text:
                all_content.append({
                    'text': text, 
                    'type': role, 
                    'page': page_number, 
                    'offset': offset
                })
                added_offsets.add(offset)

    # --- Process all content lists ---
    add_item(raw_paragraphs, 'paragraph')
    add_item(raw_tables, 'table')
    add_item(raw_figures, 'figure')
    add_item(raw_sections, 'section')

    # --- Step 3: Sort all extracted content by its character offset ---
    all_content.sort(key=lambda x: x['offset'])
    
    return all_content
