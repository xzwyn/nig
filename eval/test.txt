# src/alignment/semantic_aligner.py

from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import faiss  # For efficient nearest neighbor search
from tqdm import tqdm  # For progress bars
from openai import AzureOpenAI

import config

# Type Aliases
ContentItem = Dict[str, Any]
AlignedPair = Dict[str, Any]

# A reusable client instance
_client: Optional[AzureOpenAI] = None

def _get_azure_client() -> AzureOpenAI:
    """Initializes and returns a reusable AzureOpenAI client."""
    global _client
    if _client is None:
        print("Initializing Azure OpenAI client...")
        if not all([config.AZURE_EMBEDDING_ENDPOINT, config.AZURE_EMBEDDING_API_KEY, config.AZURE_EMBEDDING_DEPLOYMENT_NAME]):
            raise ValueError("Azure credentials (endpoint, key, deployment name) are not fully set in config.")
        _client = AzureOpenAI(
            api_version=config.AZURE_API_VERSION,
            azure_endpoint=config.AZURE_EMBEDDING_ENDPOINT,
            api_key=config.AZURE_EMBEDDING_API_KEY,
        )
    return _client

def _get_embeddings_in_batches(
    texts: List[str],
    content_items: List[ContentItem],
    client: AzureOpenAI,
    batch_size: int = 16,
    context_window: int = 1
) -> np.ndarray:
    """
    Generates embeddings by sending texts to the Azure API in batches,
    with optional context from surrounding segments.
    """
    texts_with_context = []
    print(f"Enhancing text with a context window of {context_window}...")
    for i, text in enumerate(texts):
        pre_context = " ".join([content_items[j]['text'] for j in range(max(0, i - context_window), i)])
        post_context = " ".join([content_items[j]['text'] for j in range(i + 1, min(len(texts), i + 1 + context_window))])
        content_type = content_items[i].get('type', 'unknown')
        page_num = content_items[i].get('page', 'unknown')
        context_text = f"{pre_context} [SEP] {text} [SEP] {post_context} [TYPE:{content_type}] [PAGE:{page_num}]".strip()
        texts_with_context.append(context_text)

    all_embeddings = []
    for i in tqdm(range(0, len(texts_with_context), batch_size), desc="Generating Embeddings"):
        batch = texts_with_context[i:i + batch_size]
        try:
            response = client.embeddings.create(input=batch, model=config.AZURE_EMBEDDING_DEPLOYMENT_NAME)
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
        except Exception as e:
            print(f"An error occurred while processing a batch: {e}")
            # Use a zero vector for failed batches, assuming a model dimension (e.g., 3072 for text-embedding-3-large)
            all_embeddings.extend([[0.0] * 3072] * len(batch))

    return np.array(all_embeddings, dtype='float32')

def score_candidates(x, y, ind, x_mean, y_mean, margin):
    """Helper function to calculate margin scores for candidates."""
    scores = np.zeros_like(ind, dtype=np.float32)
    for i in range(x.shape[0]):
        for j in range(ind.shape[1]):
            cosine_sim = np.dot(x[i], y[ind[i, j]])
            mean_nn_sim = (x_mean[i] + y_mean[ind[i, j]]) / 2
            scores[i, j] = margin(cosine_sim, mean_nn_sim)
    return scores

def _calculate_margin_scores_and_matches(
    source_embeds: np.ndarray,
    target_embeds: np.ndarray,
    k: int = 4
) -> List[Tuple[int, int, float]]:
    """
    Calculates alignment scores using the margin method (CSLS-like) and returns the best 1-to-1 matches.
    """
    # 1. Normalize embeddings for cosine similarity calculation via dot product
    source_norm = np.linalg.norm(source_embeds, axis=1, keepdims=True)
    target_norm = np.linalg.norm(target_embeds, axis=1, keepdims=True)
    source_embeds_norm = np.divide(source_embeds, source_norm, out=np.zeros_like(source_embeds), where=source_norm != 0)
    target_embeds_norm = np.divide(target_embeds, target_norm, out=np.zeros_like(target_embeds), where=target_norm != 0)

    # 2. Find k-Nearest Neighbors (kNN) in both directions using FAISS
    index_target = faiss.IndexFlatIP(target_embeds_norm.shape[1])
    index_target.add(target_embeds_norm)
    sim_fwd, ind_fwd = index_target.search(source_embeds_norm, k)

    index_source = faiss.IndexFlatIP(source_embeds_norm.shape[1])
    index_source.add(source_embeds_norm)
    sim_bwd, ind_bwd = index_source.search(target_embeds_norm, k)

    # 3. Calculate the margin score
    mean_fwd_sim = sim_fwd.mean(axis=1)
    mean_bwd_sim = sim_bwd.mean(axis=1)
    margin = lambda a, b: a / b if b > 0 else 0.0

    scores_fwd = score_candidates(source_embeds_norm, target_embeds_norm, ind_fwd, mean_fwd_sim, mean_bwd_sim, margin)
    scores_bwd = score_candidates(target_embeds_norm, source_embeds_norm, ind_bwd, mean_bwd_sim, mean_fwd_sim, margin)

    # 4. Find the best matches based on the new scores
    fwd_best_idx = ind_fwd[np.arange(source_embeds.shape[0]), scores_fwd.argmax(axis=1)]
    bwd_best_idx = ind_bwd[np.arange(target_embeds.shape[0]), scores_bwd.argmax(axis=1)]

    # 5. Combine and enforce a strict 1-to-1 mapping
    potential_matches = []
    fwd_scores_max = scores_fwd.max(axis=1)
    for i in range(source_embeds.shape[0]):
        potential_matches.append((i, fwd_best_idx[i], fwd_scores_max[i]))

    bwd_scores_max = scores_bwd.max(axis=1)
    for i in range(target_embeds.shape[0]):
        potential_matches.append((bwd_best_idx[i], i, bwd_scores_max[i]))

    potential_matches.sort(key=lambda x: x[2], reverse=True)

    final_matches = []
    seen_source, seen_target = set(), set()
    for src_idx, trg_idx, score in potential_matches:
        if src_idx not in seen_source and trg_idx not in seen_target:
            final_matches.append((src_idx, trg_idx, score))
            seen_source.add(src_idx)
            seen_target.add(trg_idx)
            
    return final_matches

def align_content(
    english_content: List[ContentItem],
    german_content: List[ContentItem],
    generate_debug_report: bool = False
) -> Tuple[List[AlignedPair], Optional[Dict[str, Any]]]:
    """
    Aligns content using margin-based scoring with Azure OpenAI embeddings.
    This method replaces the Hungarian algorithm for improved accuracy.

    Returns:
        A tuple containing:
        1. A list of aligned pairs.
        2. A dictionary with data for a debug report (or None if not requested).
    """
    if not english_content or not german_content:
        return [], None

    client = _get_azure_client()

    # Get embeddings for both languages
    english_embeddings = _get_embeddings_in_batches(
        [item['text'] for item in english_content], english_content, client, context_window=1
    )
    german_embeddings = _get_embeddings_in_batches(
        [item['text'] for item in german_content], german_content, client, context_window=1
    )

    # Calculate raw semantic similarity for final thresholding and reporting
    semantic_matrix = cosine_similarity(english_embeddings, german_embeddings)

    # Get the best 1-to-1 matches using the margin-based method
    print("Finding best 1-to-1 matches using margin-based scoring...")
    best_matches = _calculate_margin_scores_and_matches(english_embeddings, german_embeddings)
    
    debug_data = None
    if generate_debug_report:
        print("WARNING: Debug report for this method only contains the semantic matrix.")
        debug_data = {
            'english_content': english_content,
            'german_content': german_content,
            'semantic_matrix': semantic_matrix,
            'best_matches_with_margin_scores': best_matches
        }

    # Create the list of aligned pairs based on matches
    aligned_pairs: List[AlignedPair] = []
    used_english_indices, used_german_indices = set(), set()

    for eng_idx, ger_idx, margin_score in best_matches:
        semantic_score = semantic_matrix[eng_idx, ger_idx]
        
        # A margin score >= 1.0 indicates a likely mutual translation
        if margin_score >= 1.0 and semantic_score >= config.SIMILARITY_THRESHOLD:
            aligned_pairs.append({
                "english": english_content[eng_idx],
                "german": german_content[ger_idx],
                "similarity": float(semantic_score)
            })
            used_english_indices.add(eng_idx)
            used_german_indices.add(ger_idx)

    # Add any remaining unmatched items
    for i, item in enumerate(english_content):
        if i not in used_english_indices:
            aligned_pairs.append({"english": item, "german": None, "similarity": 0.0})
    for j, item in enumerate(german_content):
        if j not in used_german_indices:
            aligned_pairs.append({"english": None, "german": item, "similarity": 0.0})
    
    # Sort results by page number for consistent output
    def sort_key(pair):
        if pair.get('english'):
            return pair['english'].get('page', float('inf'))
        return float('inf')

    aligned_pairs.sort(key=sort_key)
    
    return aligned_pairs, debug_data
