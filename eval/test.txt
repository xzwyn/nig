# test_1/src/processing/json_parser.py
import json
from pathlib import Path
from typing import List, Dict, Any

import config

# A type alias for our structured content for clarity
ContentItem = Dict[str, Any]

def _convert_table_to_markdown(table_obj: Dict) -> str:
    """Converts an Azure table object into a Markdown string."""
    markdown_str = ""
    if not table_obj.get('cells'):
        return ""

    # Create header
    header_cells = [cell for cell in table_obj['cells'] if cell.get('kind') == 'columnHeader']
    if header_cells:
        header_cells.sort(key=lambda x: x['columnIndex'])
        # Handle cells that might span multiple columns
        header_content = []
        for cell in header_cells:
            content = cell.get('content', '').strip()
            col_span = cell.get('columnSpan', 1)
            header_content.extend([content] * col_span)

        header_row = "| " + " | ".join(header_content) + " |"
        separator_row = "| " + " | ".join(["---"] * len(header_content)) + " |"
        markdown_str += header_row + "\n" + separator_row + "\n"

    # Create body rows
    body_cells = [cell for cell in table_obj['cells'] if cell.get('kind') is None]

    rows = {}
    for cell in body_cells:
        row_idx = cell.get('rowIndex', 0)
        if row_idx not in rows:
            rows[row_idx] = []
        rows[row_idx].append(cell)

    for row_idx in sorted(rows.keys()):
        row_cells = sorted(rows[row_idx], key=lambda x: x.get('columnIndex', 0))
        row_str = "| " + " | ".join([cell.get('content', '').strip() for cell in row_cells]) + " |"
        markdown_str += row_str + "\n"

    return markdown_str.strip()


def process_document_json(doc_intelligence_data: Dict[str, Any]) -> List[ContentItem]:
    """
    Reads and processes an Azure Document Intelligence result dictionary,
    now with dedicated handling for tables.

    Args:
        doc_intelligence_data: The dictionary from Document Intelligence .to_dict().
    """
    try:
        analyze_result = doc_intelligence_data['analyzeResult']
        full_text_content = analyze_result['content']
        raw_paragraphs = analyze_result.get('paragraphs', [])
        pages = analyze_result.get('pages', [])
        raw_tables = analyze_result.get('tables', [])
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    # --- Step 1: Identify all character offsets belonging to tables to avoid duplication ---
    table_offsets = set()
    for table in raw_tables:
        for span in table.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                table_offsets.add(i)

    # Identify all character offsets that are handwritten
    handwritten_offsets = set()
    if 'styles' in analyze_result:
        for style in analyze_result['styles']:
            if style.get('isHandwritten') and style.get('spans'):
                for span in style['spans']:
                    for i in range(span['offset'], span['offset'] + span['length']):
                        handwritten_offsets.add(i)

    # Create a quick lookup for page number by span offset
    page_lookup = {}
    for page in pages:
        for span in page.get('spans', []):
            for i in range(span['offset'], span['offset'] + span['length']):
                page_lookup[i] = page.get('pageNumber', 0)

    # --- Step 2: Extract all content, including tables, and sort by position ---
    all_content: List[ContentItem] = []

    # Process PARAGRAPHS
    for p in raw_paragraphs:
        role = p.get('role', 'paragraph')
        if role in config.IGNORED_ROLES or not p.get('spans'):
            continue

        offset = p['spans'][0]['offset']
        # If the paragraph is inside a table or is handwritten, SKIP it.
        if offset in table_offsets or offset in handwritten_offsets:
            continue

        length = p['spans'][0]['length']
        text = full_text_content[offset : offset + length].strip()
        page_number = page_lookup.get(offset, 0)
        if text:
            all_content.append({'text': text, 'type': role, 'page': page_number, 'offset': offset})

    # Process TABLES
    for table in raw_tables:
        if not table.get('spans'):
            continue
        offset = table['spans'][0]['offset']
        page_number = page_lookup.get(offset, 0)
        markdown_table = _convert_table_to_markdown(table)
        if markdown_table:
            all_content.append({'text': markdown_table, 'type': 'table', 'page': page_number, 'offset': offset})

    # Sort all extracted content by its character offset to maintain document order
    all_content.sort(key=lambda x: x['offset'])

    # --- Step 3: Stitch broken paragraphs ---
    final_content: List[ContentItem] = []
    stitched_text = ""
    current_page = 0
    current_type = "paragraph"

    for i, segment in enumerate(all_content):
        # If the current element is a table or a structural heading, finalize the previous stitched text.
        is_standalone = segment['type'] in config.STRUCTURAL_ROLES or segment['type'] == 'table'

        if is_standalone:
            if stitched_text: # Finalize any pending paragraph
                final_content.append({'text': stitched_text, 'type': current_type, 'page': current_page})
                stitched_text = ""
            final_content.append(segment) # Add the standalone item
            continue

        # This logic handles stitching of regular paragraphs
        if not stitched_text: # Start a new paragraph
            stitched_text = segment['text']
            current_page = segment['page']
            current_type = segment['type']
        else:
            # If previous text ends with punctuation, start a new paragraph
            if stitched_text.endswith(('.', '!', '?', ':', '•')):
                final_content.append({'text': stitched_text, 'type': current_type, 'page': current_page})
                stitched_text = segment['text']
                current_page = segment['page']
                current_type = segment['type']
            else: # Continue stitching the current paragraph
                stitched_text += f" {segment['text']}"

    # Add the last stitched paragraph if it exists
    if stitched_text:
        final_content.append({'text': stitched_text, 'type': current_type, 'page': current_page})

    return final_content


# test_1/src/reporting/excel_writer.py
import io
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import numpy as np

import config

AlignedPair = Dict[str, Any]
EvaluationFinding = Dict[str, Any]
ContentItem = Dict[str, Any]

def save_alignment_report(aligned_data: List[AlignedPair], filepath: Path) -> None:
    """Saves the document alignment data to an Excel file."""
    if not aligned_data:
        print("Warning: No aligned data to save to Excel.")
        return
    report_data = []
    for pair in aligned_data:
        eng_item = pair.get('english')
        ger_item = pair.get('german')
        report_data.append({
            "English": eng_item.get('text', '') if eng_item else "--- OMITTED ---",
            "German": ger_item.get('text', '') if ger_item else "--- ADDED ---",
            "Similarity": f"{pair.get('similarity', 0.0):.4f}",
            "Type": (eng_item.get('type') if eng_item else ger_item.get('type', 'N/A')),
            "English Page": (eng_item.get('page') if eng_item else 'N/A'),
            "German Page": (ger_item.get('page') if ger_item else 'N/A')
        })
    df = pd.DataFrame(report_data)
    try:
        df.to_excel(filepath, index=False, engine='openpyxl')
    except Exception as e:
        print(f"Error: Could not write alignment report to '{filepath}'. Reason: {e}")


def save_evaluation_report(evaluation_results: List[EvaluationFinding], filepath: Path) -> None:
    """Saves the AI evaluation findings to a separate Excel report."""
    if not evaluation_results:
        print("No evaluation findings to save.")
        return
    evaluation_results.sort(key=lambda x: x.get('page', 0))
    df = pd.DataFrame(evaluation_results)
    desired_columns = [
        "page", "type", "suggestion", "english_text", "german_text",
        "original_phrase", "translated_phrase"
    ]
    final_columns = [col for col in desired_columns if col in df.columns]
    df = df[final_columns]
    try:
        df.to_excel(filepath, index=False, sheet_name='Evaluation_Findings')
    except Exception as e:
        print(f"Error: Could not write evaluation report to '{filepath}'. Reason: {e}")


def save_calculation_report(
    english_content: List[ContentItem],
    german_content: List[ContentItem],
    blended_matrix: np.ndarray,
    semantic_matrix: np.ndarray,
    type_matrix: np.ndarray,
    proximity_matrix: np.ndarray,
    filepath: Path
):
    """
    Saves a highly detailed, two-sheet Excel report showing all alignment score calculations.
    """
    try:
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            # --- Process English Sheet ---
            eng_report_data = []
            best_ger_indices = np.argmax(blended_matrix, axis=1)

            for i, item in enumerate(english_content):
                best_match_idx = best_ger_indices[i]
                best_match_item = german_content[best_match_idx]

                raw_semantic = semantic_matrix[i, best_match_idx]
                raw_type = type_matrix[i, best_match_idx]
                raw_proximity = proximity_matrix[i, best_match_idx]

                eng_report_data.append({
                    "Text": item['text'],
                    "Type": item['type'],
                    "Page No": item['page'],
                    "Raw Semantic": f"{raw_semantic:.4f}",
                    "Semantic Calculation": f"{raw_semantic:.4f} x {config.W_SEMANTIC}",
                    "Weighted Semantic": f"{raw_semantic * config.W_SEMANTIC:.4f}",
                    "Raw Type": f"{raw_type:.1f}",
                    "Type Calculation": f"{raw_type:.1f} x {config.W_TYPE}",
                    "Weighted Type": f"{raw_type * config.W_TYPE:.4f}",
                    "Raw Proximity": f"{raw_proximity:.4f}",
                    "Proximity Calculation": f"{raw_proximity:.4f} x {config.W_PROXIMITY}",
                    "Weighted Proximity": f"{raw_proximity * config.W_PROXIMITY:.4f}",
                    "Total Score": f"{blended_matrix[i, best_match_idx]:.4f}",
                    "Best Match (German)": best_match_item['text'],
                    "Best Match Type": best_match_item['type'],
                    "Best Match Page": best_match_item['page']
                })

            df_eng = pd.DataFrame(eng_report_data)
            df_eng.to_excel(writer, sheet_name='English Calculations', index=False)

            # --- Process German Sheet ---
            ger_report_data = []
            best_eng_indices = np.argmax(blended_matrix, axis=0)

            for j, item in enumerate(german_content):
                best_match_idx = best_eng_indices[j]
                best_match_item = english_content[best_match_idx]

                raw_semantic = semantic_matrix[best_match_idx, j]
                raw_type = type_matrix[best_match_idx, j]
                raw_proximity = proximity_matrix[best_match_idx, j]

                ger_report_data.append({
                    "Text": item['text'],
                    "Type": item['type'],
                    "Page No": item['page'],
                    "Raw Semantic": f"{raw_semantic:.4f}",
                    "Semantic Calculation": f"{raw_semantic:.4f} x {config.W_SEMANTIC}",
                    "Weighted Semantic": f"{raw_semantic * config.W_SEMANTIC:.4f}",
                    "Raw Type": f"{raw_type:.1f}",
                    "Type Calculation": f"{raw_type:.1f} x {config.W_TYPE}",
                    "Weighted Type": f"{raw_type * config.W_TYPE:.4f}",
                    "Raw Proximity": f"{raw_proximity:.4f}",
                    "Proximity Calculation": f"{raw_proximity:.4f} x {config.W_PROXIMITY}",
                    "Weighted Proximity": f"{raw_proximity * config.W_PROXIMITY:.4f}",
                    "Total Score": f"{blended_matrix[best_match_idx, j]:.4f}",
                    "Best Match (English)": best_match_item['text'],
                    "Best Match Type": best_match_item['type'],
                    "Best Match Page": best_match_item['page']
                })

            df_ger = pd.DataFrame(ger_report_data)
            df_ger.to_excel(writer, sheet_name='German Calculations', index=False)

    except Exception as e:
        print(f"Error: Could not write debug calculation report to '{filepath}'. Reason: {e}")


def create_excel_report_in_memory(evaluation_results: List[EvaluationFinding]) -> bytes:
    """
    Creates the evaluation Excel report in memory and returns it as bytes.
    """
    if not evaluation_results:
        return b'' # Return empty bytes if there's nothing to report

    evaluation_results.sort(key=lambda x: x.get('page', 0))
    df = pd.DataFrame(evaluation_results)

    # Define the desired column order for the final report
    desired_columns = [
        "page", "type", "suggestion", "english_text", "german_text",
        "original_phrase", "translated_phrase"
    ]
    # Filter the dataframe to only include existing columns in the desired order
    final_columns = [col for col in desired_columns if col in df.columns]
    df = df[final_columns]

    # Use an in-memory buffer to save the Excel file
    output_buffer = io.BytesIO()
    with pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Evaluation_Findings')

    # Retrieve the byte data from the buffer
    return output_buffer.getvalue()



# test_1/app.py
from dotenv import load_dotenv
load_dotenv()

import streamlit as st
from datetime import datetime
import time

# --- Import project modules ---
import config
from src.clients.doc_intelligence_client import analyze_pdf
from src.processing.json_parser import process_document_json
from src.alignment.semantic_aligner import align_content
from src.evaluation.pipeline import run_evaluation_pipeline
from src.reporting.excel_writer import create_excel_report_in_memory

# --- Page Configuration ---
st.set_page_config(page_title="Translation Evaluator", layout="wide")


# --- UI Functions ---
def display_results(results_list: list):
    """Renders the list of evaluation findings in the Streamlit UI."""
    if not results_list:
        return

    st.subheader(f"Found {len(results_list)} noteworthy items")
    results_list.sort(key=lambda x: x.get('page', 0))

    for result in results_list:
        error_type = result.get('type', 'Info')
        with st.container(border=True):
            st.markdown(f"**Page:** `{result.get('page', 'N/A')}` | **Type:** `{error_type}`")

            original_phrase = result.get("original_phrase")
            translated_phrase = result.get("translated_phrase")

            if original_phrase or translated_phrase:
                st.markdown("##### 🔍 Error Focus")
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Original English Phrase:**")
                    st.error(f"'{original_phrase or 'N/A'}'")
                with col2:
                    st.markdown("**Translated German Phrase:**")
                    st.warning(f"'{translated_phrase or 'N/A'}'")
                st.divider()

            st.markdown("##### Full Text Context")
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"> {result['english_text']}")
            with col2:
                st.markdown(f"> {result['german_text']}")
            st.markdown(f"**💡 Suggestion:** {result['suggestion']}")


# --- Main App ---
st.title("📚 Translation Evaluator")
st.markdown(
    "Upload a source English PDF and its German translation to identify potential "
    "omissions, additions, or mistranslations."
)
st.divider()

# --- Session State Initialization ---
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = []
if 'error_message' not in st.session_state:
    st.session_state.error_message = None


# --- Sidebar for Inputs and Controls ---
with st.sidebar:
    st.header("1. Upload Documents")
    english_pdf = st.file_uploader("Upload English PDF (Source)", type="pdf", key="eng_pdf")
    german_pdf = st.file_uploader("Upload German PDF (Translation)", type="pdf", key="ger_pdf")

    st.header("2. Configure & Run")
    # Using a fixed context window as requested
    CONTEXT_WINDOW = 1
    st.info(f"Using Hungarian Algorithm with Context Window: **{CONTEXT_WINDOW}**")

    if st.button("🚀 Run Analysis", disabled=not (english_pdf and german_pdf), type="primary"):
        # Reset state for a new run
        st.session_state.analysis_complete = False
        st.session_state.evaluation_results = []
        st.session_state.error_message = None

        try:
            # --- Main Processing Pipeline ---
            with st.spinner("Step 1/4: Analyzing English PDF with Azure Document Intelligence..."):
                eng_bytes = english_pdf.getvalue()
                eng_json_data = analyze_pdf(eng_bytes, english_pdf.name)

            with st.spinner("Step 2/4: Analyzing German PDF with Azure Document Intelligence..."):
                ger_bytes = german_pdf.getvalue()
                ger_json_data = analyze_pdf(ger_bytes, german_pdf.name)

            with st.spinner("Step 3/4: Parsing and aligning content segments..."):
                english_content = process_document_json(eng_json_data)
                german_content = process_document_json(ger_json_data)
                aligned_pairs = align_content(
                    english_content, german_content, context_window=CONTEXT_WINDOW
                )
                # Store aligned pairs if you need them for other reports later
                st.session_state.aligned_pairs = aligned_pairs

            with st.spinner("Step 4/4: Evaluating aligned pairs for translation errors..."):
                st.session_state.evaluation_results = list(run_evaluation_pipeline(aligned_pairs))

            st.session_state.analysis_complete = True
            st.success("Analysis pipeline finished successfully!")
            time.sleep(2) # Give user time to see the success message
            st.rerun()

        except Exception as e:
            st.session_state.error_message = f"An error occurred: {e}"
            st.rerun()


    # --- Download Button Logic ---
    st.header("3. Export Results")
    if st.session_state.analysis_complete and st.session_state.evaluation_results:
        excel_data = create_excel_report_in_memory(st.session_state.evaluation_results)
        current_date = datetime.now().strftime("%Y-%m-%d")
        st.download_button(
            label="📥 Download Report as Excel",
            data=excel_data,
            file_name=f"Translation_Analysis_{current_date}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        st.markdown("_Report will be available for download after a successful analysis._")


# --- Main Display Area for Results ---
st.header("Evaluation Results")

if st.session_state.error_message:
    st.error(st.session_state.error_message)
elif st.session_state.analysis_complete:
    if not st.session_state.evaluation_results:
        st.success("✅ Analysis complete. No significant errors were found.")
    else:
        display_results(st.session_state.evaluation_results)
else:
    st.info("Upload your English and German PDF files in the sidebar and click 'Run Analysis' to begin.")
