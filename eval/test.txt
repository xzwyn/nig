import json
from pathlib import Path
from typing import List, Dict, Any

import config

ContentItem = Dict[str, Any]

def _get_page_number(item: Dict[str, Any]) -> int:
    """Helper to safely get the page number from any item."""
    if 'boundingRegions' in item and item['boundingRegions']:
        return item['boundingRegions'][0].get('pageNumber', 0)
    return 0

def _get_offset(item: Dict[str, Any]) -> int:
    """Helper to safely get the offset from any item."""
    if 'spans' in item and item['spans']:
        return item['spans'][0].get('offset', 0)
    return 0

def _get_content_from_table(table: Dict[str, Any]) -> str:
    """
    Builds a simple string representation of a table.
    We are using this just for the evaluation; the Markdown is better for debug.
    For this logic, we just need *some* text.
    """
    header = " | ".join([cell.get('content', '') for cell in table.get('header', [])])
    body = "\n".join([
        " | ".join([cell.get('content', '') for cell in row.get('cells', [])])
        for row in table.get('rows', [])
    ])
    return f"Table:\n{header}\n{body}"

def process_document_json(doc_intelligence_data: Any) -> List[ContentItem]:
    """
    MODIFIED (THE FIX):
    This function now iterates through ALL content lists in the JSON:
    - paragraphs
    - tables
    - lists
    - footnotes
    It extracts their content and properties into one single, sorted list.
    """
    # Allow passing a file path or a preloaded dict
    if isinstance(doc_intelligence_data, (str, Path)):
        with open(Path(doc_intelligence_data), 'r', encoding='utf-8') as f:
            doc_intelligence_data = json.load(f)

    try:
        analyze_result = doc_intelligence_data['analyzeResult']
    except KeyError as e:
        raise ValueError(f"Document Intelligence data is missing expected key: {e}") from e

    all_content: List[ContentItem] = []

    # --- 1. Process PARAGRAPHS ---
    for p in analyze_result.get('paragraphs', []):
        role = p.get('role', 'paragraph')
        if role in config.IGNORED_ROLES:
            continue
        
        all_content.append({
            'text': p.get('content', ''),
            'type': role,
            'page': _get_page_number(p),
            'offset': _get_offset(p)
        })

    # --- 2. Process TABLES ---
    # Tables are complex. We will extract their string content.
    # Note: The 'content' field for a table in the JSON is just "<table>...</table>".
    # We can use the 'cells' to build a simpler text version if needed, or just use the content.
    for t in analyze_result.get('tables', []):
        all_content.append({
            'text': t.get('content', _get_content_from_table(t)), # Use content, fallback to manual build
            'type': 'table',
            'page': _get_page_number(t),
            'offset': _get_offset(t)
        })

    # --- 3. Process LISTS ---
    for lst in analyze_result.get('lists', []):
        # We can append the whole list as one item, or each item individually.
        # Let's add each list item as its own paragraph.
        for item in lst.get('items', []):
            all_content.append({
                'text': item.get('content', ''),
                'type': 'listItem',
                'page': _get_page_number(item),
                'offset': _get_offset(item)
            })

    # --- 4. Process FOOTNOTES ---
    for fn in analyze_result.get('footnotes', []):
        all_content.append({
            'text': fn.get('content', ''),
            'type': 'footnote',
            'page': _get_page_number(fn),
            'offset': _get_offset(fn)
        })
        
    # --- 5. Process all other document elements (as a fallback) ---
    for doc_elem in analyze_result.get('documents', []):
         for key, value in doc_elem.get('fields', {}).items():
            if value and value.get('type') == 'string':
                 all_content.append({
                    'text': value.get('content', ''),
                    'type': key, # e.g., 'documentType'
                    'page': _get_page_number(value),
                    'offset': _get_offset(value)
                })

    # --- Final Step: Sort all extracted content by its character offset ---
    all_content.sort(key=lambda x: x['offset'])
    
    # Filter out empty items that might have been added
    final_content = [item for item in all_content if item['text'] and item['text'].strip()]
    
    return final_content
